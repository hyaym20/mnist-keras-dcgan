{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from keras.layers import LeakyReLU, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 100\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "img_channels = 1\n",
    "(x_train, _), (_, _) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, img_channels)\n",
    "x_train = x_train.astype('float32')\n",
    "x_train /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator synthesizes new images from a 100-dimensional noise (uniform distribution between -1.0 to 1.0) using the inverse of convolution, called transposed convolution. Instead of fractionally-strided convolution as suggested in DCGAN, upsampling between the first three layers is used since it synthesizes more realistic handwriting images. In between layers, batch normalization stabilizes learning. The activation function after each layer is a ReLU. The output of the sigmoid at the last layer produces the fake image. Dropout of between 0.3 and 0.5 at the first layer prevents overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(): \n",
    "    dropout = 0.4\n",
    "    depth = 256 # 64+64+64+64\n",
    "    dim = 7\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 100\n",
    "    # Out: dim x dim x depth\n",
    "    model.add(Dense(dim*dim*depth, input_dim=latent_dim))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((dim, dim, depth)))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # In: dim x dim x depth\n",
    "    # Out: 2*dim x 2*dim x depth/2\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/2), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2DTranspose(int(depth/4), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2DTranspose(int(depth/8), 5, padding='same'))\n",
    "    model.add(BatchNormalization(momentum=0.9))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # Out: 28 x 28 x 1 grayscale image [0.0,1.0] per pix\n",
    "    model.add(Conv2DTranspose(1, 5, padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "A discriminator that tells how real an image is, is basically a deep Convolutional Neural Network. For MNIST Dataset, the input is an image (28 pixel x 28 pixel x 1 channel). The sigmoid output is a scalar value of the probability of how real the image is (0.0 is certainly fake, 1.0 is certainly real, anything in between is a gray area). The difference from a typical CNN is the absence of max-pooling in between layers. Instead, a strided convolution is used for downsampling. The activation function used in each CNN layer is a leaky ReLU. A dropout between 0.4 and 0.7 between layers prevent over fitting and memorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Wâˆ’F+2P)/S+1\n",
    "def discriminator_model():\n",
    "    depth = 64\n",
    "    dropout = 0.4\n",
    "    input_shape = (img_rows, img_cols, img_channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    # In: 28 x 28 x 1, depth = 1\n",
    "    # Out: 14 x 14 x 1, depth=64\n",
    "    model.add(Conv2D(depth, 5, strides=2, input_shape=input_shape, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*2, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*4, 5, strides=2, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    model.add(Conv2D(depth*8, 5, strides=1, padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(dropout))\n",
    "\n",
    "    # Out: 1-dim probability\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator model\n",
    "\n",
    "Since the output of the Discriminator is sigmoid, we use binary cross entropy for the loss. RMSProp as optimizer generates more realistic fake images compared to Adam for this case. Learning rate is 0.0008. Weight decay and clip value stabilize learning during the latter part of the training. You have to adjust the decay if you adjust the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = discriminator_model()\n",
    "discriminator.compile(loss='binary_crossentropy', \n",
    "                      optimizer=RMSprop(lr=0.0002, decay=6e-8), \n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial model\n",
    "\n",
    "The adversarial model is just the generator-discriminator stacked together. The training parameters are the same as in the Discriminator model except for a reduced learning rate and corresponding weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_model():\n",
    "    model = Sequential()\n",
    "    model.add(generator)\n",
    "    discriminator.trainable = False\n",
    "    model.add(discriminator)\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=RMSprop(lr=0.0001, decay=3e-8), \n",
    "                  metrics=['accuracy'])\n",
    "    discriminator.trainable = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial = adversarial_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(saveToFile=False, fake=True, samples=16, noise=None, epoch=0):\n",
    "    filename = 'mnist.png'\n",
    "    if fake:\n",
    "        if noise is None:\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[samples, 100])\n",
    "        else:\n",
    "            filename = \"mnist_%d.png\" % epoch\n",
    "        images = generator.predict(noise)\n",
    "    else:\n",
    "        i = np.random.randint(0, x_train.shape[0], samples)\n",
    "        images = x_train[i, :, :, :]\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [img_rows, img_cols])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if saveToFile:\n",
    "        plt.savefig(filename)\n",
    "        plt.close('all')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We determine first if Discriminator model is correct by training it alone with real and fake images. Afterwards, the Discriminator and Adversarial models are trained one after the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_epochs=2000, batch_size=256, save_interval=0):\n",
    "        noise_input = None\n",
    "        if save_interval>0:\n",
    "            noise_input = np.random.uniform(-1.0, 1.0, size=[16, 100])\n",
    "        for epoch in range(train_epochs):\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            \n",
    "            # select a random half of images\n",
    "            images_train = x_train[np.random.randint(0, x_train.shape[0], size=batch_size), :, :, :]\n",
    "            \n",
    "            # sample noise and generate a batch of new images\n",
    "            noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_dim])\n",
    "            images_fake = generator.predict(noise)\n",
    "            \n",
    "            # train the discriminator (real classified as ones and generated as zeros)\n",
    "            x = np.concatenate((images_train, images_fake))\n",
    "            y = np.ones([2*batch_size, 1])\n",
    "            y[batch_size:, :] = 0\n",
    "            d_loss = discriminator.train_on_batch(x, y)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            \n",
    "            # train the generator (wants discriminator to mistake images as real)\n",
    "            y = np.ones([batch_size, 1])\n",
    "            a_loss = adversarial.train_on_batch(noise, y)\n",
    "            \n",
    "            log_msg = \"%d: [D loss: %f, acc: %f]\" % (epoch, d_loss[0], d_loss[1])\n",
    "            log_msg = \"%s  [A loss: %f, acc: %f]\" % (log_msg, a_loss[0], a_loss[1])\n",
    "            print(log_msg)\n",
    "            if save_interval>0:\n",
    "                if (epoch+1)%save_interval==0:\n",
    "                    plot_images(saveToFile=True, samples=noise_input.shape[0],\n",
    "                                noise=noise_input, epoch=(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElapsedTimer(object):\n",
    "    def __init__(self):\n",
    "        self.start_time = time.time()\n",
    "    def elapsed(self,sec):\n",
    "        if sec < 60:\n",
    "            return str(sec) + \" sec\"\n",
    "        elif sec < (60 * 60):\n",
    "            return str(sec / 60) + \" min\"\n",
    "        else:\n",
    "            return str(sec / (60 * 60)) + \" hr\"\n",
    "    def elapsed_time(self):\n",
    "        print(\"Elapsed: %s \" % self.elapsed(time.time() - self.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [D loss: 0.693539, acc: 0.500000]  [A loss: 0.956172, acc: 0.000000]\n",
      "1: [D loss: 0.562464, acc: 0.742188]  [A loss: 2.677871, acc: 0.000000]\n",
      "2: [D loss: 0.474760, acc: 0.560547]  [A loss: 0.200526, acc: 1.000000]\n",
      "3: [D loss: 1.137499, acc: 0.500000]  [A loss: 1.798508, acc: 0.000000]\n",
      "4: [D loss: 0.380512, acc: 0.851562]  [A loss: 1.714468, acc: 0.000000]\n",
      "5: [D loss: 0.351728, acc: 0.925781]  [A loss: 1.873793, acc: 0.000000]\n",
      "6: [D loss: 0.307240, acc: 0.941406]  [A loss: 2.027952, acc: 0.000000]\n",
      "7: [D loss: 0.266801, acc: 0.960938]  [A loss: 2.292771, acc: 0.000000]\n",
      "8: [D loss: 0.245314, acc: 0.957031]  [A loss: 2.953331, acc: 0.000000]\n",
      "9: [D loss: 0.235584, acc: 0.970703]  [A loss: 4.526452, acc: 0.000000]\n",
      "10: [D loss: 0.272482, acc: 0.945312]  [A loss: 2.217271, acc: 0.000000]\n",
      "11: [D loss: 0.204847, acc: 0.996094]  [A loss: 9.157019, acc: 0.000000]\n",
      "12: [D loss: 0.780241, acc: 0.650391]  [A loss: 1.176475, acc: 0.093750]\n",
      "13: [D loss: 0.502586, acc: 0.568359]  [A loss: 4.893988, acc: 0.000000]\n",
      "14: [D loss: 0.284216, acc: 0.914062]  [A loss: 3.353813, acc: 0.000000]\n",
      "15: [D loss: 0.166617, acc: 0.996094]  [A loss: 3.614504, acc: 0.000000]\n",
      "16: [D loss: 0.144549, acc: 0.996094]  [A loss: 3.832932, acc: 0.000000]\n",
      "17: [D loss: 0.136836, acc: 0.992188]  [A loss: 3.867249, acc: 0.000000]\n",
      "18: [D loss: 0.119036, acc: 1.000000]  [A loss: 4.125916, acc: 0.000000]\n",
      "19: [D loss: 0.111987, acc: 0.988281]  [A loss: 3.887851, acc: 0.000000]\n",
      "20: [D loss: 0.093092, acc: 0.990234]  [A loss: 4.195944, acc: 0.000000]\n",
      "21: [D loss: 0.090758, acc: 0.992188]  [A loss: 4.411470, acc: 0.000000]\n",
      "22: [D loss: 0.082923, acc: 0.992188]  [A loss: 3.578621, acc: 0.000000]\n",
      "23: [D loss: 0.060096, acc: 1.000000]  [A loss: 5.196970, acc: 0.000000]\n",
      "24: [D loss: 0.098125, acc: 0.972656]  [A loss: 2.926498, acc: 0.000000]\n",
      "25: [D loss: 0.057096, acc: 1.000000]  [A loss: 6.564729, acc: 0.000000]\n",
      "26: [D loss: 0.145736, acc: 0.949219]  [A loss: 0.617680, acc: 0.679688]\n",
      "27: [D loss: 0.727599, acc: 0.560547]  [A loss: 10.296166, acc: 0.000000]\n",
      "28: [D loss: 1.496437, acc: 0.568359]  [A loss: 3.191224, acc: 0.000000]\n",
      "29: [D loss: 0.083994, acc: 0.988281]  [A loss: 2.936298, acc: 0.000000]\n",
      "30: [D loss: 0.059503, acc: 1.000000]  [A loss: 2.888866, acc: 0.000000]\n",
      "31: [D loss: 0.053728, acc: 0.998047]  [A loss: 2.787200, acc: 0.000000]\n",
      "32: [D loss: 0.051729, acc: 1.000000]  [A loss: 2.792090, acc: 0.000000]\n",
      "33: [D loss: 0.068599, acc: 0.992188]  [A loss: 2.657315, acc: 0.003906]\n",
      "34: [D loss: 0.059820, acc: 0.998047]  [A loss: 2.866794, acc: 0.000000]\n",
      "35: [D loss: 0.078018, acc: 0.986328]  [A loss: 2.589482, acc: 0.000000]\n",
      "36: [D loss: 0.072265, acc: 0.998047]  [A loss: 2.833505, acc: 0.000000]\n",
      "37: [D loss: 0.080804, acc: 0.984375]  [A loss: 2.595383, acc: 0.000000]\n",
      "38: [D loss: 0.090964, acc: 0.988281]  [A loss: 2.960237, acc: 0.000000]\n",
      "39: [D loss: 0.086148, acc: 0.988281]  [A loss: 2.723338, acc: 0.000000]\n",
      "40: [D loss: 0.087364, acc: 0.984375]  [A loss: 2.708937, acc: 0.000000]\n",
      "41: [D loss: 0.103489, acc: 0.982422]  [A loss: 2.772663, acc: 0.000000]\n",
      "42: [D loss: 0.102419, acc: 0.982422]  [A loss: 2.338282, acc: 0.015625]\n",
      "43: [D loss: 0.101551, acc: 0.986328]  [A loss: 3.445830, acc: 0.000000]\n",
      "44: [D loss: 0.115159, acc: 0.966797]  [A loss: 1.210414, acc: 0.218750]\n",
      "45: [D loss: 0.236402, acc: 0.906250]  [A loss: 7.530486, acc: 0.000000]\n",
      "46: [D loss: 1.495069, acc: 0.621094]  [A loss: 0.330151, acc: 0.914062]\n",
      "47: [D loss: 0.838778, acc: 0.535156]  [A loss: 3.230182, acc: 0.000000]\n",
      "48: [D loss: 0.231775, acc: 0.916016]  [A loss: 1.879045, acc: 0.000000]\n",
      "49: [D loss: 0.117271, acc: 0.996094]  [A loss: 1.983396, acc: 0.000000]\n",
      "50: [D loss: 0.125486, acc: 0.982422]  [A loss: 1.936166, acc: 0.003906]\n",
      "51: [D loss: 0.115323, acc: 0.992188]  [A loss: 2.170367, acc: 0.000000]\n",
      "52: [D loss: 0.161550, acc: 0.972656]  [A loss: 1.813491, acc: 0.007812]\n",
      "53: [D loss: 0.145171, acc: 0.988281]  [A loss: 2.395730, acc: 0.000000]\n",
      "54: [D loss: 0.149005, acc: 0.970703]  [A loss: 1.804443, acc: 0.003906]\n",
      "55: [D loss: 0.165053, acc: 0.980469]  [A loss: 2.427070, acc: 0.000000]\n",
      "56: [D loss: 0.179154, acc: 0.953125]  [A loss: 1.457304, acc: 0.042969]\n",
      "57: [D loss: 0.209576, acc: 0.945312]  [A loss: 3.254329, acc: 0.000000]\n",
      "58: [D loss: 0.314071, acc: 0.898438]  [A loss: 0.642923, acc: 0.621094]\n",
      "59: [D loss: 0.527543, acc: 0.625000]  [A loss: 4.447405, acc: 0.000000]\n",
      "60: [D loss: 0.803178, acc: 0.714844]  [A loss: 0.738992, acc: 0.464844]\n",
      "61: [D loss: 0.437950, acc: 0.681641]  [A loss: 2.277152, acc: 0.000000]\n",
      "62: [D loss: 0.223001, acc: 0.939453]  [A loss: 1.445643, acc: 0.015625]\n",
      "63: [D loss: 0.192791, acc: 0.984375]  [A loss: 1.938683, acc: 0.000000]\n",
      "64: [D loss: 0.219657, acc: 0.949219]  [A loss: 1.381542, acc: 0.011719]\n",
      "65: [D loss: 0.241266, acc: 0.957031]  [A loss: 1.941720, acc: 0.000000]\n",
      "66: [D loss: 0.232891, acc: 0.943359]  [A loss: 1.308273, acc: 0.031250]\n",
      "67: [D loss: 0.251675, acc: 0.966797]  [A loss: 2.239164, acc: 0.000000]\n",
      "68: [D loss: 0.284877, acc: 0.923828]  [A loss: 0.893178, acc: 0.300781]\n",
      "69: [D loss: 0.342806, acc: 0.806641]  [A loss: 3.444995, acc: 0.000000]\n",
      "70: [D loss: 0.559437, acc: 0.771484]  [A loss: 0.449071, acc: 0.871094]\n",
      "71: [D loss: 0.681520, acc: 0.523438]  [A loss: 2.510989, acc: 0.000000]\n",
      "72: [D loss: 0.352122, acc: 0.855469]  [A loss: 1.041935, acc: 0.089844]\n",
      "73: [D loss: 0.309023, acc: 0.902344]  [A loss: 1.946875, acc: 0.000000]\n",
      "74: [D loss: 0.272904, acc: 0.925781]  [A loss: 1.245254, acc: 0.035156]\n",
      "75: [D loss: 0.264199, acc: 0.953125]  [A loss: 2.016464, acc: 0.000000]\n",
      "76: [D loss: 0.260815, acc: 0.933594]  [A loss: 1.146760, acc: 0.066406]\n",
      "77: [D loss: 0.286805, acc: 0.943359]  [A loss: 2.203443, acc: 0.000000]\n",
      "78: [D loss: 0.293951, acc: 0.917969]  [A loss: 0.849306, acc: 0.285156]\n",
      "79: [D loss: 0.376727, acc: 0.753906]  [A loss: 3.086369, acc: 0.000000]\n",
      "80: [D loss: 0.541232, acc: 0.777344]  [A loss: 0.508545, acc: 0.832031]\n",
      "81: [D loss: 0.616863, acc: 0.525391]  [A loss: 2.320562, acc: 0.000000]\n",
      "82: [D loss: 0.412547, acc: 0.837891]  [A loss: 0.869242, acc: 0.246094]\n",
      "83: [D loss: 0.381399, acc: 0.759766]  [A loss: 2.031106, acc: 0.000000]\n",
      "84: [D loss: 0.360028, acc: 0.847656]  [A loss: 0.901434, acc: 0.218750]\n",
      "85: [D loss: 0.357723, acc: 0.832031]  [A loss: 2.008853, acc: 0.000000]\n",
      "86: [D loss: 0.371289, acc: 0.876953]  [A loss: 0.912838, acc: 0.218750]\n",
      "87: [D loss: 0.385274, acc: 0.791016]  [A loss: 2.154032, acc: 0.000000]\n",
      "88: [D loss: 0.409997, acc: 0.865234]  [A loss: 0.706449, acc: 0.503906]\n",
      "89: [D loss: 0.465046, acc: 0.609375]  [A loss: 2.348614, acc: 0.000000]\n",
      "90: [D loss: 0.491064, acc: 0.800781]  [A loss: 0.684648, acc: 0.542969]\n",
      "91: [D loss: 0.479782, acc: 0.611328]  [A loss: 1.935498, acc: 0.000000]\n",
      "92: [D loss: 0.349886, acc: 0.886719]  [A loss: 0.957380, acc: 0.136719]\n",
      "93: [D loss: 0.335863, acc: 0.871094]  [A loss: 1.812653, acc: 0.000000]\n",
      "94: [D loss: 0.329005, acc: 0.892578]  [A loss: 0.934937, acc: 0.132812]\n",
      "95: [D loss: 0.349774, acc: 0.861328]  [A loss: 1.968147, acc: 0.000000]\n",
      "96: [D loss: 0.390665, acc: 0.867188]  [A loss: 0.683148, acc: 0.558594]\n",
      "97: [D loss: 0.437300, acc: 0.662109]  [A loss: 2.268087, acc: 0.000000]\n",
      "98: [D loss: 0.470238, acc: 0.769531]  [A loss: 0.588825, acc: 0.718750]\n",
      "99: [D loss: 0.504473, acc: 0.576172]  [A loss: 2.034640, acc: 0.000000]\n",
      "100: [D loss: 0.427626, acc: 0.824219]  [A loss: 0.763575, acc: 0.382812]\n",
      "101: [D loss: 0.389830, acc: 0.759766]  [A loss: 1.760096, acc: 0.000000]\n",
      "102: [D loss: 0.341226, acc: 0.898438]  [A loss: 0.887692, acc: 0.191406]\n",
      "103: [D loss: 0.375668, acc: 0.835938]  [A loss: 1.731837, acc: 0.000000]\n",
      "104: [D loss: 0.363407, acc: 0.869141]  [A loss: 0.767984, acc: 0.406250]\n",
      "105: [D loss: 0.408058, acc: 0.751953]  [A loss: 2.208125, acc: 0.000000]\n",
      "106: [D loss: 0.442369, acc: 0.800781]  [A loss: 0.588578, acc: 0.730469]\n",
      "107: [D loss: 0.476201, acc: 0.603516]  [A loss: 2.092655, acc: 0.000000]\n",
      "108: [D loss: 0.456776, acc: 0.800781]  [A loss: 0.684509, acc: 0.527344]\n",
      "109: [D loss: 0.420808, acc: 0.703125]  [A loss: 1.860812, acc: 0.000000]\n",
      "110: [D loss: 0.343023, acc: 0.886719]  [A loss: 0.883817, acc: 0.246094]\n",
      "111: [D loss: 0.333319, acc: 0.878906]  [A loss: 1.780993, acc: 0.000000]\n",
      "112: [D loss: 0.352850, acc: 0.878906]  [A loss: 0.760046, acc: 0.429688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113: [D loss: 0.386682, acc: 0.777344]  [A loss: 2.274062, acc: 0.000000]\n",
      "114: [D loss: 0.527881, acc: 0.746094]  [A loss: 0.445689, acc: 0.898438]\n",
      "115: [D loss: 0.636621, acc: 0.523438]  [A loss: 2.138530, acc: 0.000000]\n",
      "116: [D loss: 0.562240, acc: 0.679688]  [A loss: 0.672262, acc: 0.542969]\n",
      "117: [D loss: 0.472387, acc: 0.644531]  [A loss: 1.574728, acc: 0.000000]\n",
      "118: [D loss: 0.362421, acc: 0.896484]  [A loss: 1.025136, acc: 0.101562]\n",
      "119: [D loss: 0.367784, acc: 0.894531]  [A loss: 1.410415, acc: 0.007812]\n",
      "120: [D loss: 0.352501, acc: 0.927734]  [A loss: 1.061459, acc: 0.089844]\n",
      "121: [D loss: 0.359084, acc: 0.894531]  [A loss: 1.601008, acc: 0.000000]\n",
      "122: [D loss: 0.363573, acc: 0.898438]  [A loss: 0.914339, acc: 0.207031]\n",
      "123: [D loss: 0.397464, acc: 0.796875]  [A loss: 2.335611, acc: 0.000000]\n",
      "124: [D loss: 0.524644, acc: 0.695312]  [A loss: 0.361207, acc: 0.957031]\n",
      "125: [D loss: 0.776260, acc: 0.509766]  [A loss: 2.279074, acc: 0.000000]\n",
      "126: [D loss: 0.568325, acc: 0.660156]  [A loss: 0.769193, acc: 0.375000]\n",
      "127: [D loss: 0.437102, acc: 0.730469]  [A loss: 1.499382, acc: 0.000000]\n",
      "128: [D loss: 0.357661, acc: 0.912109]  [A loss: 1.061400, acc: 0.078125]\n",
      "129: [D loss: 0.348280, acc: 0.933594]  [A loss: 1.511408, acc: 0.000000]\n",
      "130: [D loss: 0.330255, acc: 0.939453]  [A loss: 1.090513, acc: 0.066406]\n",
      "131: [D loss: 0.354350, acc: 0.908203]  [A loss: 1.878312, acc: 0.000000]\n",
      "132: [D loss: 0.394209, acc: 0.847656]  [A loss: 0.686774, acc: 0.562500]\n",
      "133: [D loss: 0.520166, acc: 0.593750]  [A loss: 2.756168, acc: 0.000000]\n",
      "134: [D loss: 0.851008, acc: 0.552734]  [A loss: 0.461647, acc: 0.878906]\n",
      "135: [D loss: 0.651070, acc: 0.517578]  [A loss: 1.684884, acc: 0.000000]\n",
      "136: [D loss: 0.451596, acc: 0.800781]  [A loss: 0.938007, acc: 0.179688]\n",
      "137: [D loss: 0.389467, acc: 0.865234]  [A loss: 1.439839, acc: 0.000000]\n",
      "138: [D loss: 0.386353, acc: 0.904297]  [A loss: 1.055040, acc: 0.101562]\n",
      "139: [D loss: 0.357399, acc: 0.902344]  [A loss: 1.548853, acc: 0.003906]\n",
      "140: [D loss: 0.378234, acc: 0.882812]  [A loss: 0.943718, acc: 0.207031]\n",
      "141: [D loss: 0.404208, acc: 0.796875]  [A loss: 2.046941, acc: 0.000000]\n",
      "142: [D loss: 0.469277, acc: 0.748047]  [A loss: 0.549694, acc: 0.730469]\n",
      "143: [D loss: 0.576972, acc: 0.582031]  [A loss: 2.293645, acc: 0.000000]\n",
      "144: [D loss: 0.648701, acc: 0.597656]  [A loss: 0.631330, acc: 0.632812]\n",
      "145: [D loss: 0.515758, acc: 0.589844]  [A loss: 1.681113, acc: 0.000000]\n",
      "146: [D loss: 0.413060, acc: 0.806641]  [A loss: 0.941345, acc: 0.210938]\n",
      "147: [D loss: 0.386627, acc: 0.859375]  [A loss: 1.540655, acc: 0.000000]\n",
      "148: [D loss: 0.379342, acc: 0.876953]  [A loss: 0.915980, acc: 0.257812]\n",
      "149: [D loss: 0.424171, acc: 0.794922]  [A loss: 1.997038, acc: 0.000000]\n",
      "150: [D loss: 0.474283, acc: 0.755859]  [A loss: 0.628630, acc: 0.656250]\n",
      "151: [D loss: 0.514471, acc: 0.654297]  [A loss: 2.233270, acc: 0.000000]\n",
      "152: [D loss: 0.586430, acc: 0.621094]  [A loss: 0.624613, acc: 0.585938]\n",
      "153: [D loss: 0.546325, acc: 0.619141]  [A loss: 1.882562, acc: 0.007812]\n",
      "154: [D loss: 0.504162, acc: 0.712891]  [A loss: 0.804118, acc: 0.359375]\n",
      "155: [D loss: 0.445896, acc: 0.775391]  [A loss: 1.675961, acc: 0.000000]\n",
      "156: [D loss: 0.424853, acc: 0.845703]  [A loss: 0.895077, acc: 0.292969]\n",
      "157: [D loss: 0.436599, acc: 0.775391]  [A loss: 1.817205, acc: 0.000000]\n",
      "158: [D loss: 0.462178, acc: 0.779297]  [A loss: 0.686585, acc: 0.542969]\n",
      "159: [D loss: 0.517288, acc: 0.687500]  [A loss: 2.067389, acc: 0.000000]\n",
      "160: [D loss: 0.601048, acc: 0.605469]  [A loss: 0.612111, acc: 0.648438]\n",
      "161: [D loss: 0.546095, acc: 0.615234]  [A loss: 1.810994, acc: 0.000000]\n",
      "162: [D loss: 0.541711, acc: 0.685547]  [A loss: 0.744216, acc: 0.476562]\n",
      "163: [D loss: 0.482979, acc: 0.708984]  [A loss: 1.660625, acc: 0.007812]\n",
      "164: [D loss: 0.446178, acc: 0.812500]  [A loss: 0.913157, acc: 0.332031]\n",
      "165: [D loss: 0.446585, acc: 0.785156]  [A loss: 1.689780, acc: 0.000000]\n",
      "166: [D loss: 0.462497, acc: 0.792969]  [A loss: 0.773950, acc: 0.441406]\n",
      "167: [D loss: 0.468992, acc: 0.748047]  [A loss: 1.916655, acc: 0.000000]\n",
      "168: [D loss: 0.540478, acc: 0.666016]  [A loss: 0.585234, acc: 0.699219]\n",
      "169: [D loss: 0.553903, acc: 0.646484]  [A loss: 1.946383, acc: 0.000000]\n",
      "170: [D loss: 0.537550, acc: 0.652344]  [A loss: 0.696244, acc: 0.554688]\n",
      "171: [D loss: 0.517027, acc: 0.685547]  [A loss: 1.738795, acc: 0.003906]\n",
      "172: [D loss: 0.506377, acc: 0.708984]  [A loss: 0.786673, acc: 0.414062]\n",
      "173: [D loss: 0.475906, acc: 0.728516]  [A loss: 1.679879, acc: 0.003906]\n",
      "174: [D loss: 0.476629, acc: 0.765625]  [A loss: 0.782015, acc: 0.441406]\n",
      "175: [D loss: 0.507734, acc: 0.712891]  [A loss: 1.798914, acc: 0.000000]\n",
      "176: [D loss: 0.537041, acc: 0.662109]  [A loss: 0.654147, acc: 0.589844]\n",
      "177: [D loss: 0.536728, acc: 0.664062]  [A loss: 1.824365, acc: 0.000000]\n",
      "178: [D loss: 0.539057, acc: 0.669922]  [A loss: 0.705136, acc: 0.523438]\n",
      "179: [D loss: 0.514743, acc: 0.673828]  [A loss: 1.711974, acc: 0.000000]\n",
      "180: [D loss: 0.518066, acc: 0.738281]  [A loss: 0.780809, acc: 0.464844]\n",
      "181: [D loss: 0.500106, acc: 0.728516]  [A loss: 1.640848, acc: 0.003906]\n",
      "182: [D loss: 0.500826, acc: 0.750000]  [A loss: 0.772477, acc: 0.453125]\n",
      "183: [D loss: 0.507930, acc: 0.707031]  [A loss: 1.747985, acc: 0.003906]\n",
      "184: [D loss: 0.521435, acc: 0.689453]  [A loss: 0.659849, acc: 0.570312]\n",
      "185: [D loss: 0.517433, acc: 0.681641]  [A loss: 1.809306, acc: 0.000000]\n",
      "186: [D loss: 0.568211, acc: 0.626953]  [A loss: 0.645291, acc: 0.597656]\n",
      "187: [D loss: 0.513133, acc: 0.691406]  [A loss: 1.739724, acc: 0.003906]\n",
      "188: [D loss: 0.508696, acc: 0.734375]  [A loss: 0.702981, acc: 0.535156]\n",
      "189: [D loss: 0.526513, acc: 0.695312]  [A loss: 1.767419, acc: 0.003906]\n",
      "190: [D loss: 0.549606, acc: 0.666016]  [A loss: 0.598155, acc: 0.652344]\n",
      "191: [D loss: 0.592788, acc: 0.640625]  [A loss: 1.794494, acc: 0.003906]\n",
      "192: [D loss: 0.601073, acc: 0.587891]  [A loss: 0.692983, acc: 0.546875]\n",
      "193: [D loss: 0.523499, acc: 0.689453]  [A loss: 1.471201, acc: 0.000000]\n",
      "194: [D loss: 0.502015, acc: 0.769531]  [A loss: 0.736199, acc: 0.500000]\n",
      "195: [D loss: 0.522453, acc: 0.726562]  [A loss: 1.644426, acc: 0.007812]\n",
      "196: [D loss: 0.547657, acc: 0.681641]  [A loss: 0.608655, acc: 0.667969]\n",
      "197: [D loss: 0.556097, acc: 0.656250]  [A loss: 1.710525, acc: 0.003906]\n",
      "198: [D loss: 0.586370, acc: 0.615234]  [A loss: 0.646510, acc: 0.578125]\n",
      "199: [D loss: 0.535815, acc: 0.681641]  [A loss: 1.568322, acc: 0.000000]\n",
      "200: [D loss: 0.535226, acc: 0.710938]  [A loss: 0.687959, acc: 0.574219]\n",
      "201: [D loss: 0.515440, acc: 0.699219]  [A loss: 1.542981, acc: 0.015625]\n",
      "202: [D loss: 0.517111, acc: 0.728516]  [A loss: 0.628759, acc: 0.652344]\n",
      "203: [D loss: 0.565972, acc: 0.662109]  [A loss: 1.667206, acc: 0.000000]\n",
      "204: [D loss: 0.576362, acc: 0.650391]  [A loss: 0.601759, acc: 0.667969]\n",
      "205: [D loss: 0.577990, acc: 0.611328]  [A loss: 1.627322, acc: 0.003906]\n",
      "206: [D loss: 0.584226, acc: 0.634766]  [A loss: 0.650927, acc: 0.593750]\n",
      "207: [D loss: 0.553651, acc: 0.671875]  [A loss: 1.421554, acc: 0.027344]\n",
      "208: [D loss: 0.521545, acc: 0.748047]  [A loss: 0.689566, acc: 0.531250]\n",
      "209: [D loss: 0.534032, acc: 0.724609]  [A loss: 1.495147, acc: 0.015625]\n",
      "210: [D loss: 0.555524, acc: 0.701172]  [A loss: 0.642517, acc: 0.601562]\n",
      "211: [D loss: 0.554779, acc: 0.683594]  [A loss: 1.500469, acc: 0.011719]\n",
      "212: [D loss: 0.574266, acc: 0.667969]  [A loss: 0.637113, acc: 0.621094]\n",
      "213: [D loss: 0.586481, acc: 0.656250]  [A loss: 1.561563, acc: 0.023438]\n",
      "214: [D loss: 0.599932, acc: 0.623047]  [A loss: 0.701061, acc: 0.558594]\n",
      "215: [D loss: 0.575139, acc: 0.658203]  [A loss: 1.666237, acc: 0.007812]\n",
      "216: [D loss: 0.559242, acc: 0.681641]  [A loss: 0.631870, acc: 0.632812]\n",
      "217: [D loss: 0.566668, acc: 0.669922]  [A loss: 1.646460, acc: 0.007812]\n",
      "218: [D loss: 0.609130, acc: 0.642578]  [A loss: 0.555058, acc: 0.718750]\n",
      "219: [D loss: 0.594396, acc: 0.623047]  [A loss: 1.583026, acc: 0.003906]\n",
      "220: [D loss: 0.619270, acc: 0.611328]  [A loss: 0.639066, acc: 0.625000]\n",
      "221: [D loss: 0.568823, acc: 0.646484]  [A loss: 1.371307, acc: 0.027344]\n",
      "222: [D loss: 0.564961, acc: 0.689453]  [A loss: 0.654005, acc: 0.625000]\n",
      "223: [D loss: 0.576483, acc: 0.673828]  [A loss: 1.415668, acc: 0.011719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224: [D loss: 0.595666, acc: 0.632812]  [A loss: 0.633501, acc: 0.597656]\n",
      "225: [D loss: 0.593770, acc: 0.634766]  [A loss: 1.323122, acc: 0.035156]\n",
      "226: [D loss: 0.575680, acc: 0.666016]  [A loss: 0.605157, acc: 0.660156]\n",
      "227: [D loss: 0.597288, acc: 0.625000]  [A loss: 1.485978, acc: 0.003906]\n",
      "228: [D loss: 0.609621, acc: 0.636719]  [A loss: 0.602165, acc: 0.660156]\n",
      "229: [D loss: 0.604272, acc: 0.634766]  [A loss: 1.490407, acc: 0.003906]\n",
      "230: [D loss: 0.633957, acc: 0.607422]  [A loss: 0.593126, acc: 0.679688]\n",
      "231: [D loss: 0.619158, acc: 0.593750]  [A loss: 1.404156, acc: 0.007812]\n",
      "232: [D loss: 0.619326, acc: 0.611328]  [A loss: 0.603144, acc: 0.695312]\n",
      "233: [D loss: 0.616350, acc: 0.613281]  [A loss: 1.383846, acc: 0.000000]\n",
      "234: [D loss: 0.622171, acc: 0.648438]  [A loss: 0.653296, acc: 0.605469]\n",
      "235: [D loss: 0.594664, acc: 0.648438]  [A loss: 1.285559, acc: 0.035156]\n",
      "236: [D loss: 0.585468, acc: 0.679688]  [A loss: 0.674323, acc: 0.601562]\n",
      "237: [D loss: 0.580615, acc: 0.679688]  [A loss: 1.256156, acc: 0.031250]\n",
      "238: [D loss: 0.607954, acc: 0.654297]  [A loss: 0.577514, acc: 0.730469]\n",
      "239: [D loss: 0.606513, acc: 0.605469]  [A loss: 1.482475, acc: 0.000000]\n",
      "240: [D loss: 0.660952, acc: 0.570312]  [A loss: 0.540720, acc: 0.757812]\n",
      "241: [D loss: 0.658270, acc: 0.570312]  [A loss: 1.448769, acc: 0.003906]\n",
      "242: [D loss: 0.653904, acc: 0.554688]  [A loss: 0.661558, acc: 0.582031]\n",
      "243: [D loss: 0.611817, acc: 0.621094]  [A loss: 1.178864, acc: 0.066406]\n",
      "244: [D loss: 0.592393, acc: 0.679688]  [A loss: 0.695798, acc: 0.550781]\n",
      "245: [D loss: 0.582397, acc: 0.705078]  [A loss: 1.221508, acc: 0.039062]\n",
      "246: [D loss: 0.619854, acc: 0.650391]  [A loss: 0.624633, acc: 0.632812]\n",
      "247: [D loss: 0.589841, acc: 0.638672]  [A loss: 1.343594, acc: 0.011719]\n",
      "248: [D loss: 0.617308, acc: 0.619141]  [A loss: 0.560985, acc: 0.757812]\n",
      "249: [D loss: 0.634013, acc: 0.574219]  [A loss: 1.391791, acc: 0.003906]\n",
      "250: [D loss: 0.658017, acc: 0.570312]  [A loss: 0.569427, acc: 0.746094]\n",
      "251: [D loss: 0.651202, acc: 0.582031]  [A loss: 1.288340, acc: 0.027344]\n",
      "252: [D loss: 0.652778, acc: 0.574219]  [A loss: 0.637962, acc: 0.625000]\n",
      "253: [D loss: 0.608224, acc: 0.607422]  [A loss: 1.161500, acc: 0.031250]\n",
      "254: [D loss: 0.615294, acc: 0.658203]  [A loss: 0.630691, acc: 0.644531]\n",
      "255: [D loss: 0.623691, acc: 0.634766]  [A loss: 1.250251, acc: 0.039062]\n",
      "256: [D loss: 0.641764, acc: 0.599609]  [A loss: 0.597778, acc: 0.691406]\n",
      "257: [D loss: 0.618520, acc: 0.591797]  [A loss: 1.285608, acc: 0.007812]\n",
      "258: [D loss: 0.659316, acc: 0.585938]  [A loss: 0.540480, acc: 0.765625]\n",
      "259: [D loss: 0.608895, acc: 0.625000]  [A loss: 1.291137, acc: 0.011719]\n",
      "260: [D loss: 0.649266, acc: 0.578125]  [A loss: 0.659139, acc: 0.597656]\n",
      "261: [D loss: 0.616508, acc: 0.623047]  [A loss: 1.177905, acc: 0.031250]\n",
      "262: [D loss: 0.623857, acc: 0.636719]  [A loss: 0.624909, acc: 0.644531]\n",
      "263: [D loss: 0.597516, acc: 0.662109]  [A loss: 1.175894, acc: 0.058594]\n",
      "264: [D loss: 0.625887, acc: 0.640625]  [A loss: 0.633649, acc: 0.667969]\n",
      "265: [D loss: 0.613959, acc: 0.644531]  [A loss: 1.281801, acc: 0.019531]\n",
      "266: [D loss: 0.637661, acc: 0.613281]  [A loss: 0.608574, acc: 0.667969]\n",
      "267: [D loss: 0.636100, acc: 0.591797]  [A loss: 1.292267, acc: 0.015625]\n",
      "268: [D loss: 0.638329, acc: 0.591797]  [A loss: 0.556321, acc: 0.757812]\n",
      "269: [D loss: 0.634269, acc: 0.593750]  [A loss: 1.280956, acc: 0.007812]\n",
      "270: [D loss: 0.650221, acc: 0.572266]  [A loss: 0.621394, acc: 0.660156]\n",
      "271: [D loss: 0.631091, acc: 0.605469]  [A loss: 1.129435, acc: 0.042969]\n",
      "272: [D loss: 0.613171, acc: 0.644531]  [A loss: 0.691547, acc: 0.539062]\n",
      "273: [D loss: 0.595851, acc: 0.654297]  [A loss: 1.120772, acc: 0.050781]\n",
      "274: [D loss: 0.594658, acc: 0.693359]  [A loss: 0.648004, acc: 0.609375]\n",
      "275: [D loss: 0.625852, acc: 0.628906]  [A loss: 1.302759, acc: 0.011719]\n",
      "276: [D loss: 0.651237, acc: 0.603516]  [A loss: 0.580792, acc: 0.750000]\n",
      "277: [D loss: 0.620368, acc: 0.607422]  [A loss: 1.269317, acc: 0.023438]\n",
      "278: [D loss: 0.636224, acc: 0.587891]  [A loss: 0.620160, acc: 0.675781]\n",
      "279: [D loss: 0.610467, acc: 0.603516]  [A loss: 1.204097, acc: 0.035156]\n",
      "280: [D loss: 0.631449, acc: 0.626953]  [A loss: 0.561836, acc: 0.746094]\n",
      "281: [D loss: 0.650019, acc: 0.601562]  [A loss: 1.274183, acc: 0.007812]\n",
      "282: [D loss: 0.659066, acc: 0.572266]  [A loss: 0.662535, acc: 0.597656]\n",
      "283: [D loss: 0.626417, acc: 0.625000]  [A loss: 1.112248, acc: 0.058594]\n",
      "284: [D loss: 0.622395, acc: 0.664062]  [A loss: 0.654255, acc: 0.597656]\n",
      "285: [D loss: 0.610941, acc: 0.638672]  [A loss: 1.162159, acc: 0.023438]\n",
      "286: [D loss: 0.621409, acc: 0.619141]  [A loss: 0.625338, acc: 0.652344]\n",
      "287: [D loss: 0.632782, acc: 0.609375]  [A loss: 1.192271, acc: 0.023438]\n",
      "288: [D loss: 0.625218, acc: 0.628906]  [A loss: 0.648781, acc: 0.609375]\n",
      "289: [D loss: 0.635746, acc: 0.587891]  [A loss: 1.219277, acc: 0.015625]\n",
      "290: [D loss: 0.644057, acc: 0.591797]  [A loss: 0.613152, acc: 0.718750]\n",
      "291: [D loss: 0.630233, acc: 0.601562]  [A loss: 1.198600, acc: 0.019531]\n",
      "292: [D loss: 0.639813, acc: 0.628906]  [A loss: 0.648990, acc: 0.597656]\n",
      "293: [D loss: 0.625012, acc: 0.650391]  [A loss: 1.172620, acc: 0.019531]\n",
      "294: [D loss: 0.638751, acc: 0.615234]  [A loss: 0.678291, acc: 0.554688]\n",
      "295: [D loss: 0.622192, acc: 0.630859]  [A loss: 1.088776, acc: 0.050781]\n",
      "296: [D loss: 0.607222, acc: 0.669922]  [A loss: 0.656497, acc: 0.578125]\n",
      "297: [D loss: 0.637785, acc: 0.609375]  [A loss: 1.213602, acc: 0.019531]\n",
      "298: [D loss: 0.628572, acc: 0.593750]  [A loss: 0.571443, acc: 0.742188]\n",
      "299: [D loss: 0.647685, acc: 0.583984]  [A loss: 1.296536, acc: 0.007812]\n",
      "300: [D loss: 0.672988, acc: 0.552734]  [A loss: 0.652508, acc: 0.613281]\n",
      "301: [D loss: 0.639484, acc: 0.599609]  [A loss: 1.057473, acc: 0.082031]\n",
      "302: [D loss: 0.636443, acc: 0.654297]  [A loss: 0.671343, acc: 0.570312]\n",
      "303: [D loss: 0.609249, acc: 0.666016]  [A loss: 1.051210, acc: 0.105469]\n",
      "304: [D loss: 0.609934, acc: 0.652344]  [A loss: 0.660345, acc: 0.601562]\n",
      "305: [D loss: 0.607918, acc: 0.652344]  [A loss: 1.209517, acc: 0.023438]\n",
      "306: [D loss: 0.674166, acc: 0.570312]  [A loss: 0.579005, acc: 0.761719]\n",
      "307: [D loss: 0.640204, acc: 0.597656]  [A loss: 1.194508, acc: 0.023438]\n",
      "308: [D loss: 0.642955, acc: 0.582031]  [A loss: 0.670535, acc: 0.570312]\n",
      "309: [D loss: 0.643961, acc: 0.611328]  [A loss: 1.044142, acc: 0.082031]\n",
      "310: [D loss: 0.630563, acc: 0.638672]  [A loss: 0.684922, acc: 0.558594]\n",
      "311: [D loss: 0.631086, acc: 0.638672]  [A loss: 1.064746, acc: 0.062500]\n",
      "312: [D loss: 0.627962, acc: 0.644531]  [A loss: 0.647596, acc: 0.609375]\n",
      "313: [D loss: 0.633805, acc: 0.617188]  [A loss: 1.186565, acc: 0.031250]\n",
      "314: [D loss: 0.652264, acc: 0.599609]  [A loss: 0.592737, acc: 0.703125]\n",
      "315: [D loss: 0.654987, acc: 0.583984]  [A loss: 1.239904, acc: 0.023438]\n",
      "316: [D loss: 0.666695, acc: 0.572266]  [A loss: 0.637066, acc: 0.644531]\n",
      "317: [D loss: 0.645442, acc: 0.593750]  [A loss: 1.094764, acc: 0.066406]\n",
      "318: [D loss: 0.639290, acc: 0.625000]  [A loss: 0.666152, acc: 0.558594]\n",
      "319: [D loss: 0.628539, acc: 0.667969]  [A loss: 1.068325, acc: 0.085938]\n",
      "320: [D loss: 0.629378, acc: 0.656250]  [A loss: 0.692748, acc: 0.539062]\n",
      "321: [D loss: 0.618711, acc: 0.667969]  [A loss: 0.995310, acc: 0.125000]\n",
      "322: [D loss: 0.608360, acc: 0.685547]  [A loss: 0.698134, acc: 0.535156]\n",
      "323: [D loss: 0.631229, acc: 0.646484]  [A loss: 1.085461, acc: 0.054688]\n",
      "324: [D loss: 0.637090, acc: 0.644531]  [A loss: 0.581304, acc: 0.703125]\n",
      "325: [D loss: 0.657415, acc: 0.595703]  [A loss: 1.293443, acc: 0.003906]\n",
      "326: [D loss: 0.706866, acc: 0.533203]  [A loss: 0.549781, acc: 0.765625]\n",
      "327: [D loss: 0.662549, acc: 0.587891]  [A loss: 1.081317, acc: 0.062500]\n",
      "328: [D loss: 0.630464, acc: 0.640625]  [A loss: 0.677877, acc: 0.546875]\n",
      "329: [D loss: 0.635071, acc: 0.609375]  [A loss: 0.995660, acc: 0.128906]\n",
      "330: [D loss: 0.625773, acc: 0.654297]  [A loss: 0.694702, acc: 0.539062]\n",
      "331: [D loss: 0.633702, acc: 0.650391]  [A loss: 0.996200, acc: 0.089844]\n",
      "332: [D loss: 0.636164, acc: 0.636719]  [A loss: 0.675945, acc: 0.570312]\n",
      "333: [D loss: 0.628107, acc: 0.648438]  [A loss: 1.037333, acc: 0.109375]\n",
      "334: [D loss: 0.641679, acc: 0.638672]  [A loss: 0.615159, acc: 0.652344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335: [D loss: 0.643596, acc: 0.613281]  [A loss: 1.206763, acc: 0.027344]\n",
      "336: [D loss: 0.672256, acc: 0.580078]  [A loss: 0.603296, acc: 0.695312]\n",
      "337: [D loss: 0.652526, acc: 0.595703]  [A loss: 1.165149, acc: 0.042969]\n",
      "338: [D loss: 0.660601, acc: 0.593750]  [A loss: 0.594833, acc: 0.718750]\n",
      "339: [D loss: 0.680162, acc: 0.566406]  [A loss: 1.144989, acc: 0.015625]\n",
      "340: [D loss: 0.662974, acc: 0.587891]  [A loss: 0.728677, acc: 0.437500]\n",
      "341: [D loss: 0.643265, acc: 0.644531]  [A loss: 0.974836, acc: 0.128906]\n",
      "342: [D loss: 0.630771, acc: 0.640625]  [A loss: 0.738804, acc: 0.429688]\n",
      "343: [D loss: 0.622449, acc: 0.662109]  [A loss: 0.946538, acc: 0.171875]\n",
      "344: [D loss: 0.640613, acc: 0.623047]  [A loss: 0.694506, acc: 0.558594]\n",
      "345: [D loss: 0.635268, acc: 0.652344]  [A loss: 1.010041, acc: 0.097656]\n",
      "346: [D loss: 0.636800, acc: 0.625000]  [A loss: 0.614726, acc: 0.636719]\n",
      "347: [D loss: 0.644710, acc: 0.603516]  [A loss: 1.222802, acc: 0.007812]\n",
      "348: [D loss: 0.694291, acc: 0.550781]  [A loss: 0.529484, acc: 0.824219]\n",
      "349: [D loss: 0.677013, acc: 0.580078]  [A loss: 1.169165, acc: 0.019531]\n",
      "350: [D loss: 0.670870, acc: 0.570312]  [A loss: 0.714281, acc: 0.464844]\n",
      "351: [D loss: 0.648858, acc: 0.601562]  [A loss: 0.935862, acc: 0.156250]\n",
      "352: [D loss: 0.633103, acc: 0.646484]  [A loss: 0.780598, acc: 0.394531]\n",
      "353: [D loss: 0.642757, acc: 0.660156]  [A loss: 0.838112, acc: 0.296875]\n",
      "354: [D loss: 0.625534, acc: 0.654297]  [A loss: 0.790012, acc: 0.335938]\n",
      "355: [D loss: 0.645131, acc: 0.640625]  [A loss: 0.977211, acc: 0.132812]\n",
      "356: [D loss: 0.634211, acc: 0.646484]  [A loss: 0.699926, acc: 0.535156]\n",
      "357: [D loss: 0.641109, acc: 0.632812]  [A loss: 1.107505, acc: 0.050781]\n",
      "358: [D loss: 0.648035, acc: 0.621094]  [A loss: 0.557797, acc: 0.761719]\n",
      "359: [D loss: 0.687044, acc: 0.566406]  [A loss: 1.288678, acc: 0.015625]\n",
      "360: [D loss: 0.707123, acc: 0.531250]  [A loss: 0.607206, acc: 0.667969]\n",
      "361: [D loss: 0.656932, acc: 0.578125]  [A loss: 1.028845, acc: 0.078125]\n",
      "362: [D loss: 0.649670, acc: 0.601562]  [A loss: 0.736908, acc: 0.414062]\n",
      "363: [D loss: 0.648599, acc: 0.607422]  [A loss: 0.912642, acc: 0.171875]\n",
      "364: [D loss: 0.628695, acc: 0.634766]  [A loss: 0.748330, acc: 0.417969]\n",
      "365: [D loss: 0.653863, acc: 0.599609]  [A loss: 0.905839, acc: 0.195312]\n",
      "366: [D loss: 0.633005, acc: 0.646484]  [A loss: 0.729432, acc: 0.449219]\n",
      "367: [D loss: 0.638010, acc: 0.646484]  [A loss: 1.008169, acc: 0.105469]\n",
      "368: [D loss: 0.631522, acc: 0.636719]  [A loss: 0.647865, acc: 0.601562]\n",
      "369: [D loss: 0.639769, acc: 0.634766]  [A loss: 1.159351, acc: 0.054688]\n",
      "370: [D loss: 0.664640, acc: 0.580078]  [A loss: 0.556529, acc: 0.746094]\n",
      "371: [D loss: 0.667843, acc: 0.564453]  [A loss: 1.227740, acc: 0.019531]\n",
      "372: [D loss: 0.688873, acc: 0.556641]  [A loss: 0.674334, acc: 0.535156]\n",
      "373: [D loss: 0.635378, acc: 0.623047]  [A loss: 0.960051, acc: 0.132812]\n",
      "374: [D loss: 0.627774, acc: 0.636719]  [A loss: 0.755192, acc: 0.410156]\n",
      "375: [D loss: 0.633536, acc: 0.658203]  [A loss: 0.901251, acc: 0.214844]\n",
      "376: [D loss: 0.626715, acc: 0.658203]  [A loss: 0.749845, acc: 0.453125]\n",
      "377: [D loss: 0.633196, acc: 0.675781]  [A loss: 0.935670, acc: 0.191406]\n",
      "378: [D loss: 0.628503, acc: 0.666016]  [A loss: 0.698785, acc: 0.507812]\n",
      "379: [D loss: 0.620487, acc: 0.681641]  [A loss: 1.048280, acc: 0.109375]\n",
      "380: [D loss: 0.652420, acc: 0.601562]  [A loss: 0.604099, acc: 0.675781]\n",
      "381: [D loss: 0.657310, acc: 0.591797]  [A loss: 1.243527, acc: 0.050781]\n",
      "382: [D loss: 0.685320, acc: 0.554688]  [A loss: 0.550709, acc: 0.800781]\n",
      "383: [D loss: 0.658063, acc: 0.570312]  [A loss: 1.140753, acc: 0.015625]\n",
      "384: [D loss: 0.661321, acc: 0.589844]  [A loss: 0.675136, acc: 0.523438]\n",
      "385: [D loss: 0.640906, acc: 0.634766]  [A loss: 0.960219, acc: 0.113281]\n",
      "386: [D loss: 0.634712, acc: 0.621094]  [A loss: 0.761874, acc: 0.441406]\n",
      "387: [D loss: 0.642639, acc: 0.619141]  [A loss: 0.959218, acc: 0.128906]\n",
      "388: [D loss: 0.634135, acc: 0.642578]  [A loss: 0.727488, acc: 0.453125]\n",
      "389: [D loss: 0.652911, acc: 0.613281]  [A loss: 0.999145, acc: 0.113281]\n",
      "390: [D loss: 0.645690, acc: 0.613281]  [A loss: 0.679635, acc: 0.566406]\n",
      "391: [D loss: 0.637515, acc: 0.636719]  [A loss: 1.031150, acc: 0.078125]\n",
      "392: [D loss: 0.638643, acc: 0.636719]  [A loss: 0.719564, acc: 0.539062]\n",
      "393: [D loss: 0.645766, acc: 0.630859]  [A loss: 1.006901, acc: 0.113281]\n",
      "394: [D loss: 0.631635, acc: 0.613281]  [A loss: 0.625488, acc: 0.671875]\n",
      "395: [D loss: 0.659521, acc: 0.582031]  [A loss: 1.185850, acc: 0.035156]\n",
      "396: [D loss: 0.680017, acc: 0.558594]  [A loss: 0.599844, acc: 0.718750]\n",
      "397: [D loss: 0.663914, acc: 0.582031]  [A loss: 1.054519, acc: 0.058594]\n",
      "398: [D loss: 0.658090, acc: 0.576172]  [A loss: 0.699822, acc: 0.523438]\n",
      "399: [D loss: 0.634799, acc: 0.648438]  [A loss: 0.933134, acc: 0.191406]\n",
      "400: [D loss: 0.633401, acc: 0.652344]  [A loss: 0.755448, acc: 0.441406]\n",
      "401: [D loss: 0.638555, acc: 0.660156]  [A loss: 0.945870, acc: 0.175781]\n",
      "402: [D loss: 0.646233, acc: 0.625000]  [A loss: 0.734623, acc: 0.464844]\n",
      "403: [D loss: 0.652979, acc: 0.609375]  [A loss: 1.042133, acc: 0.093750]\n",
      "404: [D loss: 0.650162, acc: 0.621094]  [A loss: 0.699757, acc: 0.507812]\n",
      "405: [D loss: 0.634610, acc: 0.630859]  [A loss: 1.024902, acc: 0.148438]\n",
      "406: [D loss: 0.640614, acc: 0.648438]  [A loss: 0.660003, acc: 0.601562]\n",
      "407: [D loss: 0.644741, acc: 0.595703]  [A loss: 1.095408, acc: 0.074219]\n",
      "408: [D loss: 0.667276, acc: 0.572266]  [A loss: 0.641367, acc: 0.632812]\n",
      "409: [D loss: 0.665348, acc: 0.585938]  [A loss: 1.077240, acc: 0.082031]\n",
      "410: [D loss: 0.678666, acc: 0.587891]  [A loss: 0.599290, acc: 0.718750]\n",
      "411: [D loss: 0.656055, acc: 0.582031]  [A loss: 1.038779, acc: 0.062500]\n",
      "412: [D loss: 0.655575, acc: 0.609375]  [A loss: 0.730455, acc: 0.464844]\n",
      "413: [D loss: 0.649897, acc: 0.603516]  [A loss: 0.912750, acc: 0.187500]\n",
      "414: [D loss: 0.623785, acc: 0.687500]  [A loss: 0.736485, acc: 0.425781]\n",
      "415: [D loss: 0.639274, acc: 0.640625]  [A loss: 0.934339, acc: 0.175781]\n",
      "416: [D loss: 0.635116, acc: 0.638672]  [A loss: 0.747622, acc: 0.394531]\n",
      "417: [D loss: 0.632422, acc: 0.648438]  [A loss: 1.003646, acc: 0.097656]\n",
      "418: [D loss: 0.641700, acc: 0.619141]  [A loss: 0.708653, acc: 0.503906]\n",
      "419: [D loss: 0.652555, acc: 0.607422]  [A loss: 1.083262, acc: 0.082031]\n",
      "420: [D loss: 0.651853, acc: 0.603516]  [A loss: 0.590061, acc: 0.738281]\n",
      "421: [D loss: 0.670022, acc: 0.566406]  [A loss: 1.205789, acc: 0.031250]\n",
      "422: [D loss: 0.662189, acc: 0.564453]  [A loss: 0.625542, acc: 0.683594]\n",
      "423: [D loss: 0.647666, acc: 0.623047]  [A loss: 0.980021, acc: 0.093750]\n",
      "424: [D loss: 0.633519, acc: 0.634766]  [A loss: 0.732102, acc: 0.429688]\n",
      "425: [D loss: 0.641635, acc: 0.638672]  [A loss: 0.946443, acc: 0.160156]\n",
      "426: [D loss: 0.648516, acc: 0.644531]  [A loss: 0.781551, acc: 0.363281]\n",
      "427: [D loss: 0.646142, acc: 0.644531]  [A loss: 0.918870, acc: 0.210938]\n",
      "428: [D loss: 0.614656, acc: 0.687500]  [A loss: 0.756837, acc: 0.402344]\n",
      "429: [D loss: 0.647061, acc: 0.611328]  [A loss: 1.051492, acc: 0.113281]\n",
      "430: [D loss: 0.631156, acc: 0.650391]  [A loss: 0.677966, acc: 0.550781]\n",
      "431: [D loss: 0.649975, acc: 0.605469]  [A loss: 1.244310, acc: 0.019531]\n",
      "432: [D loss: 0.671151, acc: 0.560547]  [A loss: 0.598190, acc: 0.699219]\n",
      "433: [D loss: 0.671964, acc: 0.572266]  [A loss: 1.164709, acc: 0.035156]\n",
      "434: [D loss: 0.665659, acc: 0.589844]  [A loss: 0.680764, acc: 0.535156]\n",
      "435: [D loss: 0.630096, acc: 0.658203]  [A loss: 0.935969, acc: 0.167969]\n",
      "436: [D loss: 0.623637, acc: 0.656250]  [A loss: 0.806556, acc: 0.324219]\n",
      "437: [D loss: 0.628125, acc: 0.662109]  [A loss: 0.855731, acc: 0.277344]\n",
      "438: [D loss: 0.639035, acc: 0.642578]  [A loss: 0.853885, acc: 0.269531]\n",
      "439: [D loss: 0.645095, acc: 0.630859]  [A loss: 0.865084, acc: 0.250000]\n",
      "440: [D loss: 0.626524, acc: 0.677734]  [A loss: 0.828434, acc: 0.304688]\n",
      "441: [D loss: 0.646407, acc: 0.623047]  [A loss: 0.884789, acc: 0.242188]\n",
      "442: [D loss: 0.636818, acc: 0.626953]  [A loss: 0.839014, acc: 0.335938]\n",
      "443: [D loss: 0.638816, acc: 0.636719]  [A loss: 0.910802, acc: 0.242188]\n",
      "444: [D loss: 0.627035, acc: 0.683594]  [A loss: 0.730583, acc: 0.421875]\n",
      "445: [D loss: 0.654664, acc: 0.599609]  [A loss: 1.177250, acc: 0.039062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446: [D loss: 0.669111, acc: 0.585938]  [A loss: 0.560560, acc: 0.757812]\n",
      "447: [D loss: 0.687883, acc: 0.542969]  [A loss: 1.256462, acc: 0.027344]\n",
      "448: [D loss: 0.685903, acc: 0.550781]  [A loss: 0.629171, acc: 0.640625]\n",
      "449: [D loss: 0.651666, acc: 0.593750]  [A loss: 0.995144, acc: 0.125000]\n",
      "450: [D loss: 0.662650, acc: 0.607422]  [A loss: 0.757551, acc: 0.402344]\n",
      "451: [D loss: 0.640205, acc: 0.623047]  [A loss: 0.855874, acc: 0.277344]\n",
      "452: [D loss: 0.632585, acc: 0.644531]  [A loss: 0.817666, acc: 0.328125]\n",
      "453: [D loss: 0.642405, acc: 0.628906]  [A loss: 0.880526, acc: 0.226562]\n",
      "454: [D loss: 0.633849, acc: 0.652344]  [A loss: 0.790705, acc: 0.363281]\n",
      "455: [D loss: 0.634682, acc: 0.660156]  [A loss: 0.866527, acc: 0.285156]\n",
      "456: [D loss: 0.640182, acc: 0.642578]  [A loss: 0.980631, acc: 0.160156]\n",
      "457: [D loss: 0.652448, acc: 0.621094]  [A loss: 0.738720, acc: 0.457031]\n",
      "458: [D loss: 0.642246, acc: 0.623047]  [A loss: 0.983707, acc: 0.164062]\n",
      "459: [D loss: 0.652074, acc: 0.623047]  [A loss: 0.653911, acc: 0.617188]\n",
      "460: [D loss: 0.649770, acc: 0.603516]  [A loss: 1.118584, acc: 0.074219]\n",
      "461: [D loss: 0.655580, acc: 0.582031]  [A loss: 0.625661, acc: 0.664062]\n",
      "462: [D loss: 0.676608, acc: 0.576172]  [A loss: 1.152821, acc: 0.050781]\n",
      "463: [D loss: 0.638509, acc: 0.599609]  [A loss: 0.693710, acc: 0.496094]\n",
      "464: [D loss: 0.642016, acc: 0.638672]  [A loss: 0.983500, acc: 0.164062]\n",
      "465: [D loss: 0.644435, acc: 0.623047]  [A loss: 0.775734, acc: 0.343750]\n",
      "466: [D loss: 0.623662, acc: 0.660156]  [A loss: 0.937314, acc: 0.160156]\n",
      "467: [D loss: 0.629722, acc: 0.660156]  [A loss: 0.795726, acc: 0.363281]\n",
      "468: [D loss: 0.634975, acc: 0.625000]  [A loss: 0.990970, acc: 0.140625]\n",
      "469: [D loss: 0.641421, acc: 0.619141]  [A loss: 0.731314, acc: 0.460938]\n",
      "470: [D loss: 0.631599, acc: 0.621094]  [A loss: 1.126641, acc: 0.082031]\n",
      "471: [D loss: 0.653524, acc: 0.583984]  [A loss: 0.659019, acc: 0.585938]\n",
      "472: [D loss: 0.643068, acc: 0.613281]  [A loss: 1.101331, acc: 0.066406]\n",
      "473: [D loss: 0.650661, acc: 0.599609]  [A loss: 0.747143, acc: 0.429688]\n",
      "474: [D loss: 0.637807, acc: 0.632812]  [A loss: 0.969760, acc: 0.152344]\n",
      "475: [D loss: 0.655047, acc: 0.611328]  [A loss: 0.797127, acc: 0.347656]\n",
      "476: [D loss: 0.647431, acc: 0.619141]  [A loss: 0.954142, acc: 0.132812]\n",
      "477: [D loss: 0.653708, acc: 0.591797]  [A loss: 0.840115, acc: 0.292969]\n",
      "478: [D loss: 0.625200, acc: 0.662109]  [A loss: 0.874531, acc: 0.261719]\n",
      "479: [D loss: 0.646774, acc: 0.652344]  [A loss: 0.880096, acc: 0.246094]\n",
      "480: [D loss: 0.623052, acc: 0.648438]  [A loss: 0.834390, acc: 0.328125]\n",
      "481: [D loss: 0.656177, acc: 0.615234]  [A loss: 0.917569, acc: 0.207031]\n",
      "482: [D loss: 0.620469, acc: 0.667969]  [A loss: 0.808632, acc: 0.343750]\n",
      "483: [D loss: 0.643996, acc: 0.621094]  [A loss: 0.945803, acc: 0.207031]\n",
      "484: [D loss: 0.640282, acc: 0.642578]  [A loss: 0.732200, acc: 0.484375]\n",
      "485: [D loss: 0.631516, acc: 0.625000]  [A loss: 1.117501, acc: 0.062500]\n",
      "486: [D loss: 0.626970, acc: 0.664062]  [A loss: 0.607556, acc: 0.648438]\n",
      "487: [D loss: 0.670658, acc: 0.593750]  [A loss: 1.297606, acc: 0.035156]\n",
      "488: [D loss: 0.698681, acc: 0.556641]  [A loss: 0.641345, acc: 0.617188]\n",
      "489: [D loss: 0.657294, acc: 0.570312]  [A loss: 1.007730, acc: 0.085938]\n",
      "490: [D loss: 0.640319, acc: 0.654297]  [A loss: 0.751779, acc: 0.437500]\n",
      "491: [D loss: 0.625245, acc: 0.630859]  [A loss: 0.922477, acc: 0.191406]\n",
      "492: [D loss: 0.632592, acc: 0.644531]  [A loss: 0.843418, acc: 0.285156]\n",
      "493: [D loss: 0.609728, acc: 0.666016]  [A loss: 0.882892, acc: 0.250000]\n",
      "494: [D loss: 0.634342, acc: 0.625000]  [A loss: 0.865007, acc: 0.296875]\n",
      "495: [D loss: 0.612852, acc: 0.687500]  [A loss: 0.862954, acc: 0.250000]\n",
      "496: [D loss: 0.633894, acc: 0.625000]  [A loss: 0.914966, acc: 0.253906]\n",
      "497: [D loss: 0.633059, acc: 0.632812]  [A loss: 0.864503, acc: 0.292969]\n",
      "498: [D loss: 0.619733, acc: 0.677734]  [A loss: 0.848723, acc: 0.281250]\n",
      "499: [D loss: 0.621410, acc: 0.669922]  [A loss: 0.887551, acc: 0.289062]\n",
      "500: [D loss: 0.614602, acc: 0.667969]  [A loss: 0.932696, acc: 0.246094]\n",
      "501: [D loss: 0.633656, acc: 0.640625]  [A loss: 0.806726, acc: 0.378906]\n",
      "502: [D loss: 0.633716, acc: 0.648438]  [A loss: 1.019665, acc: 0.183594]\n",
      "503: [D loss: 0.643315, acc: 0.625000]  [A loss: 0.576592, acc: 0.734375]\n",
      "504: [D loss: 0.694248, acc: 0.582031]  [A loss: 1.477113, acc: 0.007812]\n",
      "505: [D loss: 0.739644, acc: 0.537109]  [A loss: 0.654317, acc: 0.617188]\n",
      "506: [D loss: 0.643592, acc: 0.597656]  [A loss: 0.964458, acc: 0.152344]\n",
      "507: [D loss: 0.613098, acc: 0.681641]  [A loss: 0.805296, acc: 0.347656]\n",
      "508: [D loss: 0.627709, acc: 0.646484]  [A loss: 0.879290, acc: 0.234375]\n",
      "509: [D loss: 0.639750, acc: 0.636719]  [A loss: 0.763107, acc: 0.445312]\n",
      "510: [D loss: 0.640627, acc: 0.638672]  [A loss: 0.954132, acc: 0.203125]\n",
      "511: [D loss: 0.627832, acc: 0.660156]  [A loss: 0.761114, acc: 0.425781]\n",
      "512: [D loss: 0.652606, acc: 0.595703]  [A loss: 0.965560, acc: 0.179688]\n",
      "513: [D loss: 0.654520, acc: 0.626953]  [A loss: 0.736319, acc: 0.484375]\n",
      "514: [D loss: 0.611063, acc: 0.673828]  [A loss: 0.958872, acc: 0.175781]\n",
      "515: [D loss: 0.634770, acc: 0.656250]  [A loss: 0.729529, acc: 0.488281]\n",
      "516: [D loss: 0.649970, acc: 0.585938]  [A loss: 1.096801, acc: 0.105469]\n",
      "517: [D loss: 0.649816, acc: 0.605469]  [A loss: 0.732422, acc: 0.488281]\n",
      "518: [D loss: 0.627366, acc: 0.648438]  [A loss: 1.056067, acc: 0.105469]\n",
      "519: [D loss: 0.631230, acc: 0.652344]  [A loss: 0.712806, acc: 0.492188]\n",
      "520: [D loss: 0.628500, acc: 0.636719]  [A loss: 1.055661, acc: 0.109375]\n",
      "521: [D loss: 0.623337, acc: 0.642578]  [A loss: 0.657070, acc: 0.613281]\n",
      "522: [D loss: 0.629063, acc: 0.619141]  [A loss: 1.120615, acc: 0.113281]\n",
      "523: [D loss: 0.652455, acc: 0.607422]  [A loss: 0.693769, acc: 0.550781]\n",
      "524: [D loss: 0.618679, acc: 0.666016]  [A loss: 1.062201, acc: 0.109375]\n",
      "525: [D loss: 0.637654, acc: 0.636719]  [A loss: 0.774311, acc: 0.378906]\n",
      "526: [D loss: 0.627180, acc: 0.660156]  [A loss: 1.001266, acc: 0.132812]\n",
      "527: [D loss: 0.622774, acc: 0.644531]  [A loss: 0.732331, acc: 0.480469]\n",
      "528: [D loss: 0.642414, acc: 0.646484]  [A loss: 1.124937, acc: 0.093750]\n",
      "529: [D loss: 0.675786, acc: 0.568359]  [A loss: 0.696030, acc: 0.539062]\n",
      "530: [D loss: 0.643583, acc: 0.615234]  [A loss: 1.059885, acc: 0.113281]\n",
      "531: [D loss: 0.637599, acc: 0.636719]  [A loss: 0.757751, acc: 0.390625]\n",
      "532: [D loss: 0.640759, acc: 0.644531]  [A loss: 1.000553, acc: 0.156250]\n",
      "533: [D loss: 0.637208, acc: 0.644531]  [A loss: 0.804059, acc: 0.355469]\n",
      "534: [D loss: 0.613855, acc: 0.667969]  [A loss: 0.958394, acc: 0.171875]\n",
      "535: [D loss: 0.632110, acc: 0.660156]  [A loss: 0.739554, acc: 0.457031]\n",
      "536: [D loss: 0.632558, acc: 0.632812]  [A loss: 1.030220, acc: 0.113281]\n",
      "537: [D loss: 0.641760, acc: 0.630859]  [A loss: 0.737418, acc: 0.464844]\n",
      "538: [D loss: 0.627154, acc: 0.625000]  [A loss: 1.071369, acc: 0.082031]\n",
      "539: [D loss: 0.635922, acc: 0.625000]  [A loss: 0.735050, acc: 0.453125]\n",
      "540: [D loss: 0.633015, acc: 0.625000]  [A loss: 1.066648, acc: 0.113281]\n",
      "541: [D loss: 0.609869, acc: 0.662109]  [A loss: 0.684908, acc: 0.546875]\n",
      "542: [D loss: 0.683243, acc: 0.580078]  [A loss: 1.176103, acc: 0.062500]\n",
      "543: [D loss: 0.660014, acc: 0.609375]  [A loss: 0.741724, acc: 0.457031]\n",
      "544: [D loss: 0.639406, acc: 0.609375]  [A loss: 1.016747, acc: 0.132812]\n",
      "545: [D loss: 0.631180, acc: 0.644531]  [A loss: 0.798140, acc: 0.402344]\n",
      "546: [D loss: 0.619229, acc: 0.656250]  [A loss: 0.926025, acc: 0.179688]\n",
      "547: [D loss: 0.628103, acc: 0.630859]  [A loss: 0.836415, acc: 0.359375]\n",
      "548: [D loss: 0.614836, acc: 0.669922]  [A loss: 0.937310, acc: 0.218750]\n",
      "549: [D loss: 0.633423, acc: 0.664062]  [A loss: 0.805918, acc: 0.355469]\n",
      "550: [D loss: 0.638077, acc: 0.634766]  [A loss: 1.032293, acc: 0.140625]\n",
      "551: [D loss: 0.630095, acc: 0.650391]  [A loss: 0.689066, acc: 0.550781]\n",
      "552: [D loss: 0.658452, acc: 0.615234]  [A loss: 1.202021, acc: 0.062500]\n",
      "553: [D loss: 0.660149, acc: 0.589844]  [A loss: 0.675773, acc: 0.570312]\n",
      "554: [D loss: 0.656298, acc: 0.605469]  [A loss: 1.055860, acc: 0.109375]\n",
      "555: [D loss: 0.638832, acc: 0.615234]  [A loss: 0.716455, acc: 0.449219]\n",
      "556: [D loss: 0.647423, acc: 0.615234]  [A loss: 1.051011, acc: 0.101562]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557: [D loss: 0.646170, acc: 0.613281]  [A loss: 0.810025, acc: 0.382812]\n",
      "558: [D loss: 0.611659, acc: 0.693359]  [A loss: 0.953145, acc: 0.214844]\n",
      "559: [D loss: 0.638246, acc: 0.644531]  [A loss: 0.851961, acc: 0.312500]\n",
      "560: [D loss: 0.649944, acc: 0.621094]  [A loss: 0.908290, acc: 0.222656]\n",
      "561: [D loss: 0.641289, acc: 0.628906]  [A loss: 0.831644, acc: 0.347656]\n",
      "562: [D loss: 0.629363, acc: 0.644531]  [A loss: 0.989557, acc: 0.171875]\n",
      "563: [D loss: 0.634185, acc: 0.648438]  [A loss: 0.791022, acc: 0.378906]\n",
      "564: [D loss: 0.615703, acc: 0.660156]  [A loss: 1.048377, acc: 0.136719]\n",
      "565: [D loss: 0.637846, acc: 0.636719]  [A loss: 0.719162, acc: 0.503906]\n",
      "566: [D loss: 0.636875, acc: 0.630859]  [A loss: 1.116626, acc: 0.074219]\n",
      "567: [D loss: 0.647944, acc: 0.619141]  [A loss: 0.744988, acc: 0.511719]\n",
      "568: [D loss: 0.668266, acc: 0.562500]  [A loss: 0.990476, acc: 0.140625]\n",
      "569: [D loss: 0.629325, acc: 0.648438]  [A loss: 0.759700, acc: 0.445312]\n",
      "570: [D loss: 0.643833, acc: 0.605469]  [A loss: 1.023087, acc: 0.171875]\n",
      "571: [D loss: 0.625133, acc: 0.664062]  [A loss: 0.767143, acc: 0.410156]\n",
      "572: [D loss: 0.649463, acc: 0.621094]  [A loss: 1.096189, acc: 0.105469]\n",
      "573: [D loss: 0.626240, acc: 0.644531]  [A loss: 0.767854, acc: 0.414062]\n",
      "574: [D loss: 0.642824, acc: 0.625000]  [A loss: 1.027157, acc: 0.128906]\n",
      "575: [D loss: 0.630137, acc: 0.640625]  [A loss: 0.808576, acc: 0.320312]\n",
      "576: [D loss: 0.615046, acc: 0.656250]  [A loss: 0.966797, acc: 0.222656]\n",
      "577: [D loss: 0.619951, acc: 0.660156]  [A loss: 0.772130, acc: 0.425781]\n",
      "578: [D loss: 0.614467, acc: 0.652344]  [A loss: 0.997681, acc: 0.171875]\n",
      "579: [D loss: 0.625880, acc: 0.638672]  [A loss: 0.778970, acc: 0.445312]\n",
      "580: [D loss: 0.618960, acc: 0.658203]  [A loss: 1.024586, acc: 0.183594]\n",
      "581: [D loss: 0.617176, acc: 0.662109]  [A loss: 0.748602, acc: 0.441406]\n",
      "582: [D loss: 0.642480, acc: 0.619141]  [A loss: 1.231530, acc: 0.050781]\n",
      "583: [D loss: 0.644098, acc: 0.621094]  [A loss: 0.675254, acc: 0.535156]\n",
      "584: [D loss: 0.648518, acc: 0.609375]  [A loss: 1.163303, acc: 0.085938]\n",
      "585: [D loss: 0.620692, acc: 0.652344]  [A loss: 0.738670, acc: 0.472656]\n",
      "586: [D loss: 0.630331, acc: 0.654297]  [A loss: 1.038070, acc: 0.148438]\n",
      "587: [D loss: 0.635444, acc: 0.654297]  [A loss: 0.766184, acc: 0.429688]\n",
      "588: [D loss: 0.608055, acc: 0.656250]  [A loss: 1.013930, acc: 0.195312]\n",
      "589: [D loss: 0.629872, acc: 0.638672]  [A loss: 0.791807, acc: 0.394531]\n",
      "590: [D loss: 0.644515, acc: 0.615234]  [A loss: 1.087272, acc: 0.136719]\n",
      "591: [D loss: 0.653187, acc: 0.619141]  [A loss: 0.763166, acc: 0.425781]\n",
      "592: [D loss: 0.631905, acc: 0.623047]  [A loss: 1.046020, acc: 0.128906]\n",
      "593: [D loss: 0.610848, acc: 0.669922]  [A loss: 0.731706, acc: 0.457031]\n",
      "594: [D loss: 0.621103, acc: 0.660156]  [A loss: 1.133223, acc: 0.089844]\n",
      "595: [D loss: 0.610477, acc: 0.666016]  [A loss: 0.732219, acc: 0.500000]\n",
      "596: [D loss: 0.645871, acc: 0.623047]  [A loss: 1.162980, acc: 0.078125]\n",
      "597: [D loss: 0.640829, acc: 0.630859]  [A loss: 0.774580, acc: 0.417969]\n",
      "598: [D loss: 0.611312, acc: 0.638672]  [A loss: 1.035094, acc: 0.128906]\n",
      "599: [D loss: 0.616558, acc: 0.675781]  [A loss: 0.853417, acc: 0.292969]\n",
      "600: [D loss: 0.641412, acc: 0.640625]  [A loss: 0.991992, acc: 0.183594]\n",
      "601: [D loss: 0.635888, acc: 0.652344]  [A loss: 0.835569, acc: 0.320312]\n",
      "602: [D loss: 0.616267, acc: 0.658203]  [A loss: 1.059942, acc: 0.152344]\n",
      "603: [D loss: 0.632663, acc: 0.667969]  [A loss: 0.757976, acc: 0.437500]\n",
      "604: [D loss: 0.629220, acc: 0.648438]  [A loss: 1.068039, acc: 0.136719]\n",
      "605: [D loss: 0.630723, acc: 0.654297]  [A loss: 0.734576, acc: 0.507812]\n",
      "606: [D loss: 0.635770, acc: 0.632812]  [A loss: 1.021585, acc: 0.167969]\n",
      "607: [D loss: 0.636082, acc: 0.615234]  [A loss: 0.775207, acc: 0.398438]\n",
      "608: [D loss: 0.635688, acc: 0.626953]  [A loss: 1.033370, acc: 0.144531]\n",
      "609: [D loss: 0.617970, acc: 0.654297]  [A loss: 0.765578, acc: 0.398438]\n",
      "610: [D loss: 0.622956, acc: 0.646484]  [A loss: 1.082818, acc: 0.109375]\n",
      "611: [D loss: 0.612864, acc: 0.662109]  [A loss: 0.717492, acc: 0.519531]\n",
      "612: [D loss: 0.639323, acc: 0.623047]  [A loss: 1.141944, acc: 0.093750]\n",
      "613: [D loss: 0.639931, acc: 0.628906]  [A loss: 0.718858, acc: 0.492188]\n",
      "614: [D loss: 0.663914, acc: 0.585938]  [A loss: 1.165864, acc: 0.074219]\n",
      "615: [D loss: 0.644948, acc: 0.626953]  [A loss: 0.760609, acc: 0.414062]\n",
      "616: [D loss: 0.620804, acc: 0.628906]  [A loss: 1.048028, acc: 0.121094]\n",
      "617: [D loss: 0.624726, acc: 0.634766]  [A loss: 0.792398, acc: 0.386719]\n",
      "618: [D loss: 0.637801, acc: 0.628906]  [A loss: 1.010530, acc: 0.144531]\n",
      "619: [D loss: 0.642924, acc: 0.619141]  [A loss: 0.850100, acc: 0.304688]\n",
      "620: [D loss: 0.619478, acc: 0.646484]  [A loss: 0.934770, acc: 0.226562]\n",
      "621: [D loss: 0.623905, acc: 0.658203]  [A loss: 0.845228, acc: 0.355469]\n",
      "622: [D loss: 0.625622, acc: 0.626953]  [A loss: 1.031019, acc: 0.160156]\n",
      "623: [D loss: 0.610802, acc: 0.656250]  [A loss: 0.822143, acc: 0.378906]\n",
      "624: [D loss: 0.639779, acc: 0.632812]  [A loss: 1.068450, acc: 0.125000]\n",
      "625: [D loss: 0.648639, acc: 0.642578]  [A loss: 0.770235, acc: 0.457031]\n",
      "626: [D loss: 0.639500, acc: 0.619141]  [A loss: 1.095448, acc: 0.121094]\n",
      "627: [D loss: 0.638341, acc: 0.611328]  [A loss: 0.787458, acc: 0.417969]\n",
      "628: [D loss: 0.623739, acc: 0.658203]  [A loss: 1.090574, acc: 0.121094]\n",
      "629: [D loss: 0.611182, acc: 0.667969]  [A loss: 0.792206, acc: 0.406250]\n",
      "630: [D loss: 0.629912, acc: 0.667969]  [A loss: 1.084064, acc: 0.160156]\n",
      "631: [D loss: 0.623237, acc: 0.664062]  [A loss: 0.778643, acc: 0.394531]\n",
      "632: [D loss: 0.628742, acc: 0.642578]  [A loss: 1.080134, acc: 0.144531]\n",
      "633: [D loss: 0.635811, acc: 0.644531]  [A loss: 0.746554, acc: 0.460938]\n",
      "634: [D loss: 0.631748, acc: 0.636719]  [A loss: 1.085094, acc: 0.132812]\n",
      "635: [D loss: 0.638567, acc: 0.626953]  [A loss: 0.775053, acc: 0.406250]\n",
      "636: [D loss: 0.616564, acc: 0.675781]  [A loss: 1.105588, acc: 0.125000]\n",
      "637: [D loss: 0.603588, acc: 0.673828]  [A loss: 0.773106, acc: 0.425781]\n",
      "638: [D loss: 0.646393, acc: 0.648438]  [A loss: 1.101289, acc: 0.089844]\n",
      "639: [D loss: 0.625919, acc: 0.636719]  [A loss: 0.740538, acc: 0.445312]\n",
      "640: [D loss: 0.634811, acc: 0.638672]  [A loss: 1.013844, acc: 0.171875]\n",
      "641: [D loss: 0.613038, acc: 0.667969]  [A loss: 0.805723, acc: 0.394531]\n",
      "642: [D loss: 0.598404, acc: 0.695312]  [A loss: 1.019848, acc: 0.175781]\n",
      "643: [D loss: 0.599703, acc: 0.703125]  [A loss: 0.843001, acc: 0.367188]\n",
      "644: [D loss: 0.616653, acc: 0.658203]  [A loss: 0.984784, acc: 0.183594]\n",
      "645: [D loss: 0.637572, acc: 0.640625]  [A loss: 0.884583, acc: 0.304688]\n",
      "646: [D loss: 0.618227, acc: 0.667969]  [A loss: 0.916657, acc: 0.269531]\n",
      "647: [D loss: 0.617699, acc: 0.660156]  [A loss: 0.970406, acc: 0.214844]\n",
      "648: [D loss: 0.629483, acc: 0.666016]  [A loss: 0.892586, acc: 0.246094]\n",
      "649: [D loss: 0.611000, acc: 0.693359]  [A loss: 0.973524, acc: 0.234375]\n",
      "650: [D loss: 0.605453, acc: 0.683594]  [A loss: 0.905943, acc: 0.257812]\n",
      "651: [D loss: 0.592394, acc: 0.699219]  [A loss: 0.999706, acc: 0.214844]\n",
      "652: [D loss: 0.597790, acc: 0.689453]  [A loss: 0.939606, acc: 0.246094]\n",
      "653: [D loss: 0.631803, acc: 0.628906]  [A loss: 1.086980, acc: 0.164062]\n",
      "654: [D loss: 0.619847, acc: 0.656250]  [A loss: 0.715684, acc: 0.507812]\n",
      "655: [D loss: 0.633991, acc: 0.630859]  [A loss: 1.445357, acc: 0.019531]\n",
      "656: [D loss: 0.684208, acc: 0.582031]  [A loss: 0.592557, acc: 0.671875]\n",
      "657: [D loss: 0.655594, acc: 0.593750]  [A loss: 1.186844, acc: 0.078125]\n",
      "658: [D loss: 0.645254, acc: 0.623047]  [A loss: 0.766057, acc: 0.406250]\n",
      "659: [D loss: 0.613969, acc: 0.662109]  [A loss: 1.058061, acc: 0.121094]\n",
      "660: [D loss: 0.618839, acc: 0.673828]  [A loss: 0.905170, acc: 0.261719]\n",
      "661: [D loss: 0.603951, acc: 0.675781]  [A loss: 0.924712, acc: 0.246094]\n",
      "662: [D loss: 0.607670, acc: 0.685547]  [A loss: 0.911872, acc: 0.242188]\n",
      "663: [D loss: 0.629369, acc: 0.646484]  [A loss: 0.926731, acc: 0.261719]\n",
      "664: [D loss: 0.627475, acc: 0.640625]  [A loss: 0.955343, acc: 0.230469]\n",
      "665: [D loss: 0.603113, acc: 0.699219]  [A loss: 0.887600, acc: 0.320312]\n",
      "666: [D loss: 0.632128, acc: 0.640625]  [A loss: 1.039286, acc: 0.175781]\n",
      "667: [D loss: 0.611023, acc: 0.691406]  [A loss: 0.919904, acc: 0.296875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668: [D loss: 0.616934, acc: 0.625000]  [A loss: 0.931803, acc: 0.273438]\n",
      "669: [D loss: 0.644003, acc: 0.628906]  [A loss: 0.969275, acc: 0.234375]\n",
      "670: [D loss: 0.598340, acc: 0.677734]  [A loss: 0.816346, acc: 0.363281]\n",
      "671: [D loss: 0.603416, acc: 0.695312]  [A loss: 1.180246, acc: 0.082031]\n",
      "672: [D loss: 0.635935, acc: 0.638672]  [A loss: 0.664989, acc: 0.562500]\n",
      "673: [D loss: 0.636071, acc: 0.630859]  [A loss: 1.226608, acc: 0.054688]\n",
      "674: [D loss: 0.656289, acc: 0.587891]  [A loss: 0.752298, acc: 0.468750]\n",
      "675: [D loss: 0.639611, acc: 0.626953]  [A loss: 1.092704, acc: 0.093750]\n",
      "676: [D loss: 0.621002, acc: 0.656250]  [A loss: 0.792616, acc: 0.425781]\n",
      "677: [D loss: 0.614039, acc: 0.667969]  [A loss: 1.030976, acc: 0.152344]\n",
      "678: [D loss: 0.614120, acc: 0.664062]  [A loss: 0.870143, acc: 0.324219]\n",
      "679: [D loss: 0.614285, acc: 0.687500]  [A loss: 0.997979, acc: 0.199219]\n",
      "680: [D loss: 0.631558, acc: 0.660156]  [A loss: 0.843658, acc: 0.378906]\n",
      "681: [D loss: 0.597871, acc: 0.677734]  [A loss: 1.080806, acc: 0.160156]\n",
      "682: [D loss: 0.601483, acc: 0.689453]  [A loss: 0.832045, acc: 0.367188]\n",
      "683: [D loss: 0.632089, acc: 0.642578]  [A loss: 1.186360, acc: 0.066406]\n",
      "684: [D loss: 0.655783, acc: 0.613281]  [A loss: 0.743559, acc: 0.476562]\n",
      "685: [D loss: 0.624910, acc: 0.650391]  [A loss: 1.086705, acc: 0.125000]\n",
      "686: [D loss: 0.608802, acc: 0.664062]  [A loss: 0.825887, acc: 0.378906]\n",
      "687: [D loss: 0.584754, acc: 0.681641]  [A loss: 0.972543, acc: 0.226562]\n",
      "688: [D loss: 0.570683, acc: 0.732422]  [A loss: 0.905548, acc: 0.265625]\n",
      "689: [D loss: 0.617844, acc: 0.648438]  [A loss: 1.096763, acc: 0.167969]\n",
      "690: [D loss: 0.615257, acc: 0.648438]  [A loss: 0.740913, acc: 0.496094]\n",
      "691: [D loss: 0.622045, acc: 0.644531]  [A loss: 1.232901, acc: 0.070312]\n",
      "692: [D loss: 0.641835, acc: 0.621094]  [A loss: 0.710457, acc: 0.515625]\n",
      "693: [D loss: 0.645581, acc: 0.628906]  [A loss: 1.204697, acc: 0.097656]\n",
      "694: [D loss: 0.618203, acc: 0.648438]  [A loss: 0.785704, acc: 0.421875]\n",
      "695: [D loss: 0.616652, acc: 0.652344]  [A loss: 1.065927, acc: 0.167969]\n",
      "696: [D loss: 0.649695, acc: 0.626953]  [A loss: 0.840965, acc: 0.398438]\n",
      "697: [D loss: 0.619905, acc: 0.664062]  [A loss: 1.048739, acc: 0.136719]\n",
      "698: [D loss: 0.594261, acc: 0.708984]  [A loss: 0.828548, acc: 0.347656]\n",
      "699: [D loss: 0.596596, acc: 0.699219]  [A loss: 1.025748, acc: 0.210938]\n",
      "700: [D loss: 0.625295, acc: 0.652344]  [A loss: 0.889864, acc: 0.277344]\n",
      "701: [D loss: 0.616968, acc: 0.666016]  [A loss: 1.018494, acc: 0.230469]\n",
      "702: [D loss: 0.594299, acc: 0.699219]  [A loss: 0.917010, acc: 0.277344]\n",
      "703: [D loss: 0.609236, acc: 0.650391]  [A loss: 0.985950, acc: 0.199219]\n",
      "704: [D loss: 0.604943, acc: 0.703125]  [A loss: 1.036724, acc: 0.179688]\n",
      "705: [D loss: 0.592049, acc: 0.701172]  [A loss: 0.869233, acc: 0.347656]\n",
      "706: [D loss: 0.617673, acc: 0.646484]  [A loss: 1.146866, acc: 0.144531]\n",
      "707: [D loss: 0.624588, acc: 0.664062]  [A loss: 0.781336, acc: 0.445312]\n",
      "708: [D loss: 0.635156, acc: 0.632812]  [A loss: 1.301153, acc: 0.082031]\n",
      "709: [D loss: 0.639176, acc: 0.625000]  [A loss: 0.694866, acc: 0.550781]\n",
      "710: [D loss: 0.632163, acc: 0.638672]  [A loss: 1.217654, acc: 0.093750]\n",
      "711: [D loss: 0.657581, acc: 0.611328]  [A loss: 0.770499, acc: 0.457031]\n",
      "712: [D loss: 0.618442, acc: 0.656250]  [A loss: 1.090197, acc: 0.156250]\n",
      "713: [D loss: 0.601230, acc: 0.701172]  [A loss: 0.894005, acc: 0.296875]\n",
      "714: [D loss: 0.619345, acc: 0.654297]  [A loss: 1.032757, acc: 0.171875]\n",
      "715: [D loss: 0.606687, acc: 0.683594]  [A loss: 0.888504, acc: 0.296875]\n",
      "716: [D loss: 0.591446, acc: 0.701172]  [A loss: 0.995937, acc: 0.191406]\n",
      "717: [D loss: 0.633493, acc: 0.634766]  [A loss: 0.872557, acc: 0.296875]\n",
      "718: [D loss: 0.597123, acc: 0.695312]  [A loss: 1.127431, acc: 0.140625]\n",
      "719: [D loss: 0.615735, acc: 0.650391]  [A loss: 0.821808, acc: 0.398438]\n",
      "720: [D loss: 0.634009, acc: 0.644531]  [A loss: 1.200330, acc: 0.078125]\n",
      "721: [D loss: 0.623185, acc: 0.640625]  [A loss: 0.741561, acc: 0.492188]\n",
      "722: [D loss: 0.628793, acc: 0.650391]  [A loss: 1.103290, acc: 0.140625]\n",
      "723: [D loss: 0.616124, acc: 0.648438]  [A loss: 0.838645, acc: 0.339844]\n",
      "724: [D loss: 0.634806, acc: 0.632812]  [A loss: 1.110366, acc: 0.132812]\n",
      "725: [D loss: 0.607263, acc: 0.664062]  [A loss: 0.856249, acc: 0.359375]\n",
      "726: [D loss: 0.619490, acc: 0.669922]  [A loss: 1.119494, acc: 0.144531]\n",
      "727: [D loss: 0.610452, acc: 0.675781]  [A loss: 0.858757, acc: 0.335938]\n",
      "728: [D loss: 0.632341, acc: 0.652344]  [A loss: 1.154997, acc: 0.109375]\n",
      "729: [D loss: 0.623696, acc: 0.644531]  [A loss: 0.855631, acc: 0.289062]\n",
      "730: [D loss: 0.600635, acc: 0.699219]  [A loss: 1.108005, acc: 0.152344]\n",
      "731: [D loss: 0.612383, acc: 0.656250]  [A loss: 0.895393, acc: 0.273438]\n",
      "732: [D loss: 0.622158, acc: 0.648438]  [A loss: 1.013757, acc: 0.191406]\n",
      "733: [D loss: 0.612106, acc: 0.666016]  [A loss: 1.009908, acc: 0.183594]\n",
      "734: [D loss: 0.622149, acc: 0.662109]  [A loss: 0.872181, acc: 0.308594]\n",
      "735: [D loss: 0.588432, acc: 0.691406]  [A loss: 1.083697, acc: 0.160156]\n",
      "736: [D loss: 0.600765, acc: 0.681641]  [A loss: 0.905518, acc: 0.304688]\n",
      "737: [D loss: 0.622316, acc: 0.654297]  [A loss: 1.102058, acc: 0.148438]\n",
      "738: [D loss: 0.637842, acc: 0.658203]  [A loss: 0.829726, acc: 0.355469]\n",
      "739: [D loss: 0.589253, acc: 0.707031]  [A loss: 1.120351, acc: 0.128906]\n",
      "740: [D loss: 0.615374, acc: 0.664062]  [A loss: 0.787675, acc: 0.441406]\n",
      "741: [D loss: 0.607366, acc: 0.646484]  [A loss: 1.254633, acc: 0.089844]\n",
      "742: [D loss: 0.647272, acc: 0.611328]  [A loss: 0.759452, acc: 0.449219]\n",
      "743: [D loss: 0.647048, acc: 0.628906]  [A loss: 1.141453, acc: 0.113281]\n",
      "744: [D loss: 0.632134, acc: 0.656250]  [A loss: 0.839777, acc: 0.410156]\n",
      "745: [D loss: 0.600785, acc: 0.695312]  [A loss: 1.106404, acc: 0.152344]\n",
      "746: [D loss: 0.602198, acc: 0.687500]  [A loss: 0.896617, acc: 0.312500]\n",
      "747: [D loss: 0.613159, acc: 0.685547]  [A loss: 0.987311, acc: 0.203125]\n",
      "748: [D loss: 0.625544, acc: 0.638672]  [A loss: 0.955154, acc: 0.246094]\n",
      "749: [D loss: 0.616490, acc: 0.666016]  [A loss: 0.956885, acc: 0.277344]\n",
      "750: [D loss: 0.625612, acc: 0.669922]  [A loss: 1.001467, acc: 0.175781]\n",
      "751: [D loss: 0.623355, acc: 0.652344]  [A loss: 0.920320, acc: 0.246094]\n",
      "752: [D loss: 0.642856, acc: 0.623047]  [A loss: 1.066167, acc: 0.167969]\n",
      "753: [D loss: 0.597144, acc: 0.675781]  [A loss: 0.814803, acc: 0.394531]\n",
      "754: [D loss: 0.619213, acc: 0.644531]  [A loss: 1.324164, acc: 0.070312]\n",
      "755: [D loss: 0.650594, acc: 0.640625]  [A loss: 0.693548, acc: 0.570312]\n",
      "756: [D loss: 0.625239, acc: 0.642578]  [A loss: 1.219878, acc: 0.085938]\n",
      "757: [D loss: 0.633705, acc: 0.638672]  [A loss: 0.801189, acc: 0.414062]\n",
      "758: [D loss: 0.637109, acc: 0.625000]  [A loss: 1.074155, acc: 0.156250]\n",
      "759: [D loss: 0.611135, acc: 0.677734]  [A loss: 0.893488, acc: 0.265625]\n",
      "760: [D loss: 0.616714, acc: 0.654297]  [A loss: 1.019997, acc: 0.191406]\n",
      "761: [D loss: 0.621482, acc: 0.669922]  [A loss: 0.930439, acc: 0.246094]\n",
      "762: [D loss: 0.601041, acc: 0.677734]  [A loss: 0.961445, acc: 0.277344]\n",
      "763: [D loss: 0.626578, acc: 0.640625]  [A loss: 0.994146, acc: 0.222656]\n",
      "764: [D loss: 0.628503, acc: 0.642578]  [A loss: 0.913961, acc: 0.285156]\n",
      "765: [D loss: 0.606982, acc: 0.671875]  [A loss: 1.035081, acc: 0.199219]\n",
      "766: [D loss: 0.609392, acc: 0.679688]  [A loss: 0.855403, acc: 0.308594]\n",
      "767: [D loss: 0.599613, acc: 0.671875]  [A loss: 1.054259, acc: 0.199219]\n",
      "768: [D loss: 0.600049, acc: 0.679688]  [A loss: 0.810395, acc: 0.382812]\n",
      "769: [D loss: 0.601775, acc: 0.683594]  [A loss: 1.296803, acc: 0.074219]\n",
      "770: [D loss: 0.630526, acc: 0.638672]  [A loss: 0.747019, acc: 0.503906]\n",
      "771: [D loss: 0.676266, acc: 0.599609]  [A loss: 1.230242, acc: 0.066406]\n",
      "772: [D loss: 0.638863, acc: 0.615234]  [A loss: 0.835110, acc: 0.363281]\n",
      "773: [D loss: 0.595447, acc: 0.695312]  [A loss: 1.059916, acc: 0.175781]\n",
      "774: [D loss: 0.617173, acc: 0.652344]  [A loss: 0.916678, acc: 0.289062]\n",
      "775: [D loss: 0.615026, acc: 0.681641]  [A loss: 1.103643, acc: 0.121094]\n",
      "776: [D loss: 0.602610, acc: 0.695312]  [A loss: 0.897485, acc: 0.281250]\n",
      "777: [D loss: 0.608371, acc: 0.658203]  [A loss: 1.044236, acc: 0.187500]\n",
      "778: [D loss: 0.629158, acc: 0.638672]  [A loss: 0.883361, acc: 0.316406]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779: [D loss: 0.610653, acc: 0.685547]  [A loss: 1.021777, acc: 0.210938]\n",
      "780: [D loss: 0.615307, acc: 0.664062]  [A loss: 0.939581, acc: 0.308594]\n",
      "781: [D loss: 0.617495, acc: 0.640625]  [A loss: 1.114431, acc: 0.125000]\n",
      "782: [D loss: 0.586599, acc: 0.718750]  [A loss: 0.822694, acc: 0.371094]\n",
      "783: [D loss: 0.630162, acc: 0.646484]  [A loss: 1.193369, acc: 0.117188]\n",
      "784: [D loss: 0.636074, acc: 0.662109]  [A loss: 0.801607, acc: 0.421875]\n",
      "785: [D loss: 0.623777, acc: 0.666016]  [A loss: 1.129213, acc: 0.144531]\n",
      "786: [D loss: 0.620660, acc: 0.664062]  [A loss: 0.834948, acc: 0.382812]\n",
      "787: [D loss: 0.609194, acc: 0.666016]  [A loss: 1.100695, acc: 0.144531]\n",
      "788: [D loss: 0.612077, acc: 0.660156]  [A loss: 0.856646, acc: 0.347656]\n",
      "789: [D loss: 0.594409, acc: 0.675781]  [A loss: 1.154695, acc: 0.160156]\n",
      "790: [D loss: 0.617921, acc: 0.685547]  [A loss: 0.937597, acc: 0.292969]\n",
      "791: [D loss: 0.618236, acc: 0.644531]  [A loss: 1.161213, acc: 0.113281]\n",
      "792: [D loss: 0.617496, acc: 0.644531]  [A loss: 0.752164, acc: 0.472656]\n",
      "793: [D loss: 0.617740, acc: 0.650391]  [A loss: 1.217244, acc: 0.082031]\n",
      "794: [D loss: 0.637087, acc: 0.638672]  [A loss: 0.838006, acc: 0.390625]\n",
      "795: [D loss: 0.621735, acc: 0.664062]  [A loss: 1.104359, acc: 0.132812]\n",
      "796: [D loss: 0.592194, acc: 0.707031]  [A loss: 0.892519, acc: 0.285156]\n",
      "797: [D loss: 0.615177, acc: 0.648438]  [A loss: 1.037736, acc: 0.191406]\n",
      "798: [D loss: 0.605834, acc: 0.652344]  [A loss: 0.817133, acc: 0.414062]\n",
      "799: [D loss: 0.621268, acc: 0.673828]  [A loss: 1.083438, acc: 0.160156]\n",
      "800: [D loss: 0.613279, acc: 0.658203]  [A loss: 0.827452, acc: 0.386719]\n",
      "801: [D loss: 0.660722, acc: 0.609375]  [A loss: 1.241649, acc: 0.093750]\n",
      "802: [D loss: 0.624916, acc: 0.662109]  [A loss: 0.840421, acc: 0.363281]\n",
      "803: [D loss: 0.606300, acc: 0.652344]  [A loss: 1.077023, acc: 0.167969]\n",
      "804: [D loss: 0.586102, acc: 0.708984]  [A loss: 0.932919, acc: 0.250000]\n",
      "805: [D loss: 0.612569, acc: 0.673828]  [A loss: 1.110027, acc: 0.140625]\n",
      "806: [D loss: 0.625649, acc: 0.667969]  [A loss: 0.936752, acc: 0.253906]\n",
      "807: [D loss: 0.608873, acc: 0.685547]  [A loss: 1.002244, acc: 0.203125]\n",
      "808: [D loss: 0.598862, acc: 0.707031]  [A loss: 0.936193, acc: 0.281250]\n",
      "809: [D loss: 0.613019, acc: 0.685547]  [A loss: 1.120123, acc: 0.156250]\n",
      "810: [D loss: 0.593643, acc: 0.683594]  [A loss: 0.892856, acc: 0.289062]\n",
      "811: [D loss: 0.608505, acc: 0.648438]  [A loss: 1.208071, acc: 0.082031]\n",
      "812: [D loss: 0.621271, acc: 0.660156]  [A loss: 0.822316, acc: 0.394531]\n",
      "813: [D loss: 0.606304, acc: 0.638672]  [A loss: 1.148075, acc: 0.140625]\n",
      "814: [D loss: 0.604399, acc: 0.693359]  [A loss: 0.851401, acc: 0.324219]\n",
      "815: [D loss: 0.633318, acc: 0.660156]  [A loss: 1.234147, acc: 0.078125]\n",
      "816: [D loss: 0.636884, acc: 0.630859]  [A loss: 0.850278, acc: 0.351562]\n",
      "817: [D loss: 0.614876, acc: 0.669922]  [A loss: 1.015291, acc: 0.214844]\n",
      "818: [D loss: 0.619503, acc: 0.681641]  [A loss: 0.932920, acc: 0.277344]\n",
      "819: [D loss: 0.592099, acc: 0.683594]  [A loss: 1.017848, acc: 0.199219]\n",
      "820: [D loss: 0.621138, acc: 0.640625]  [A loss: 0.924917, acc: 0.261719]\n",
      "821: [D loss: 0.585625, acc: 0.708984]  [A loss: 0.975521, acc: 0.246094]\n",
      "822: [D loss: 0.582269, acc: 0.695312]  [A loss: 0.948467, acc: 0.277344]\n",
      "823: [D loss: 0.588934, acc: 0.689453]  [A loss: 1.035532, acc: 0.207031]\n",
      "824: [D loss: 0.585302, acc: 0.728516]  [A loss: 0.963153, acc: 0.265625]\n",
      "825: [D loss: 0.604466, acc: 0.671875]  [A loss: 1.039133, acc: 0.214844]\n",
      "826: [D loss: 0.592775, acc: 0.710938]  [A loss: 0.922003, acc: 0.289062]\n",
      "827: [D loss: 0.606457, acc: 0.664062]  [A loss: 1.044844, acc: 0.230469]\n",
      "828: [D loss: 0.619078, acc: 0.644531]  [A loss: 0.910759, acc: 0.363281]\n",
      "829: [D loss: 0.618560, acc: 0.679688]  [A loss: 1.157182, acc: 0.128906]\n",
      "830: [D loss: 0.610773, acc: 0.658203]  [A loss: 0.721363, acc: 0.496094]\n",
      "831: [D loss: 0.640652, acc: 0.638672]  [A loss: 1.365516, acc: 0.082031]\n",
      "832: [D loss: 0.643435, acc: 0.632812]  [A loss: 0.720959, acc: 0.535156]\n",
      "833: [D loss: 0.614980, acc: 0.640625]  [A loss: 1.105125, acc: 0.140625]\n",
      "834: [D loss: 0.601937, acc: 0.673828]  [A loss: 0.806056, acc: 0.390625]\n",
      "835: [D loss: 0.648838, acc: 0.638672]  [A loss: 1.205745, acc: 0.085938]\n",
      "836: [D loss: 0.641171, acc: 0.652344]  [A loss: 0.794382, acc: 0.445312]\n",
      "837: [D loss: 0.600740, acc: 0.675781]  [A loss: 1.106109, acc: 0.140625]\n",
      "838: [D loss: 0.605474, acc: 0.652344]  [A loss: 0.903291, acc: 0.289062]\n",
      "839: [D loss: 0.607853, acc: 0.693359]  [A loss: 1.003600, acc: 0.234375]\n",
      "840: [D loss: 0.617468, acc: 0.662109]  [A loss: 0.930696, acc: 0.277344]\n",
      "841: [D loss: 0.619543, acc: 0.652344]  [A loss: 1.065127, acc: 0.191406]\n",
      "842: [D loss: 0.571293, acc: 0.714844]  [A loss: 0.929860, acc: 0.289062]\n",
      "843: [D loss: 0.590622, acc: 0.662109]  [A loss: 1.171155, acc: 0.117188]\n",
      "844: [D loss: 0.611860, acc: 0.648438]  [A loss: 0.867823, acc: 0.367188]\n",
      "845: [D loss: 0.605535, acc: 0.656250]  [A loss: 1.030131, acc: 0.195312]\n",
      "846: [D loss: 0.597211, acc: 0.683594]  [A loss: 0.908773, acc: 0.308594]\n",
      "847: [D loss: 0.586026, acc: 0.701172]  [A loss: 0.981024, acc: 0.257812]\n",
      "848: [D loss: 0.596110, acc: 0.679688]  [A loss: 0.905781, acc: 0.261719]\n",
      "849: [D loss: 0.630140, acc: 0.650391]  [A loss: 1.026680, acc: 0.234375]\n",
      "850: [D loss: 0.589680, acc: 0.691406]  [A loss: 0.974720, acc: 0.261719]\n",
      "851: [D loss: 0.592213, acc: 0.695312]  [A loss: 1.123485, acc: 0.171875]\n",
      "852: [D loss: 0.598230, acc: 0.689453]  [A loss: 0.934857, acc: 0.308594]\n",
      "853: [D loss: 0.622175, acc: 0.660156]  [A loss: 1.278196, acc: 0.109375]\n",
      "854: [D loss: 0.617982, acc: 0.658203]  [A loss: 0.732338, acc: 0.468750]\n",
      "855: [D loss: 0.614924, acc: 0.677734]  [A loss: 1.342223, acc: 0.054688]\n",
      "856: [D loss: 0.629855, acc: 0.623047]  [A loss: 0.731842, acc: 0.503906]\n",
      "857: [D loss: 0.628889, acc: 0.640625]  [A loss: 1.237358, acc: 0.058594]\n",
      "858: [D loss: 0.621318, acc: 0.662109]  [A loss: 0.904663, acc: 0.300781]\n",
      "859: [D loss: 0.601528, acc: 0.675781]  [A loss: 1.060877, acc: 0.148438]\n",
      "860: [D loss: 0.604884, acc: 0.679688]  [A loss: 0.988425, acc: 0.210938]\n",
      "861: [D loss: 0.595782, acc: 0.675781]  [A loss: 1.032577, acc: 0.238281]\n",
      "862: [D loss: 0.603860, acc: 0.664062]  [A loss: 0.988091, acc: 0.253906]\n",
      "863: [D loss: 0.564644, acc: 0.708984]  [A loss: 1.056131, acc: 0.199219]\n",
      "864: [D loss: 0.624199, acc: 0.636719]  [A loss: 0.969877, acc: 0.242188]\n",
      "865: [D loss: 0.587194, acc: 0.691406]  [A loss: 0.914817, acc: 0.304688]\n",
      "866: [D loss: 0.603452, acc: 0.677734]  [A loss: 1.185399, acc: 0.144531]\n",
      "867: [D loss: 0.612436, acc: 0.656250]  [A loss: 0.832131, acc: 0.371094]\n",
      "868: [D loss: 0.640384, acc: 0.628906]  [A loss: 1.275456, acc: 0.078125]\n",
      "869: [D loss: 0.612272, acc: 0.650391]  [A loss: 0.798901, acc: 0.437500]\n",
      "870: [D loss: 0.604681, acc: 0.685547]  [A loss: 1.131456, acc: 0.140625]\n",
      "871: [D loss: 0.647152, acc: 0.615234]  [A loss: 0.869540, acc: 0.347656]\n",
      "872: [D loss: 0.588593, acc: 0.703125]  [A loss: 1.175562, acc: 0.136719]\n",
      "873: [D loss: 0.608369, acc: 0.664062]  [A loss: 0.886760, acc: 0.339844]\n",
      "874: [D loss: 0.561096, acc: 0.724609]  [A loss: 1.040097, acc: 0.242188]\n",
      "875: [D loss: 0.612462, acc: 0.654297]  [A loss: 0.916492, acc: 0.312500]\n",
      "876: [D loss: 0.607189, acc: 0.689453]  [A loss: 1.022359, acc: 0.222656]\n",
      "877: [D loss: 0.618245, acc: 0.644531]  [A loss: 0.962121, acc: 0.246094]\n",
      "878: [D loss: 0.599425, acc: 0.677734]  [A loss: 0.919032, acc: 0.250000]\n",
      "879: [D loss: 0.608746, acc: 0.677734]  [A loss: 1.302835, acc: 0.097656]\n",
      "880: [D loss: 0.641889, acc: 0.625000]  [A loss: 0.714681, acc: 0.511719]\n",
      "881: [D loss: 0.643088, acc: 0.623047]  [A loss: 1.356650, acc: 0.039062]\n",
      "882: [D loss: 0.635638, acc: 0.626953]  [A loss: 0.803864, acc: 0.363281]\n",
      "883: [D loss: 0.587360, acc: 0.671875]  [A loss: 1.106100, acc: 0.164062]\n",
      "884: [D loss: 0.585412, acc: 0.675781]  [A loss: 0.990916, acc: 0.238281]\n",
      "885: [D loss: 0.613646, acc: 0.654297]  [A loss: 1.027187, acc: 0.218750]\n",
      "886: [D loss: 0.607667, acc: 0.675781]  [A loss: 1.047327, acc: 0.207031]\n",
      "887: [D loss: 0.619832, acc: 0.652344]  [A loss: 0.925343, acc: 0.304688]\n",
      "888: [D loss: 0.596784, acc: 0.708984]  [A loss: 1.153515, acc: 0.144531]\n",
      "889: [D loss: 0.602505, acc: 0.656250]  [A loss: 0.810021, acc: 0.390625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890: [D loss: 0.621539, acc: 0.660156]  [A loss: 1.207441, acc: 0.089844]\n",
      "891: [D loss: 0.616724, acc: 0.636719]  [A loss: 0.776940, acc: 0.468750]\n",
      "892: [D loss: 0.595741, acc: 0.685547]  [A loss: 1.179262, acc: 0.101562]\n",
      "893: [D loss: 0.610534, acc: 0.669922]  [A loss: 0.973553, acc: 0.261719]\n",
      "894: [D loss: 0.629461, acc: 0.652344]  [A loss: 1.156368, acc: 0.144531]\n",
      "895: [D loss: 0.578226, acc: 0.703125]  [A loss: 0.992990, acc: 0.296875]\n",
      "896: [D loss: 0.615979, acc: 0.679688]  [A loss: 1.156049, acc: 0.140625]\n",
      "897: [D loss: 0.609650, acc: 0.677734]  [A loss: 0.913324, acc: 0.304688]\n",
      "898: [D loss: 0.566464, acc: 0.720703]  [A loss: 1.086759, acc: 0.140625]\n",
      "899: [D loss: 0.607564, acc: 0.650391]  [A loss: 0.887457, acc: 0.308594]\n",
      "900: [D loss: 0.609609, acc: 0.667969]  [A loss: 1.290769, acc: 0.089844]\n",
      "901: [D loss: 0.634436, acc: 0.642578]  [A loss: 0.776270, acc: 0.476562]\n",
      "902: [D loss: 0.622481, acc: 0.648438]  [A loss: 1.103941, acc: 0.167969]\n",
      "903: [D loss: 0.615410, acc: 0.609375]  [A loss: 0.841569, acc: 0.359375]\n",
      "904: [D loss: 0.589072, acc: 0.679688]  [A loss: 1.124727, acc: 0.144531]\n",
      "905: [D loss: 0.603377, acc: 0.656250]  [A loss: 0.982442, acc: 0.253906]\n",
      "906: [D loss: 0.611725, acc: 0.677734]  [A loss: 0.926878, acc: 0.281250]\n",
      "907: [D loss: 0.590024, acc: 0.669922]  [A loss: 1.020936, acc: 0.195312]\n",
      "908: [D loss: 0.587110, acc: 0.714844]  [A loss: 0.949878, acc: 0.281250]\n",
      "909: [D loss: 0.614765, acc: 0.664062]  [A loss: 1.078893, acc: 0.183594]\n",
      "910: [D loss: 0.596256, acc: 0.693359]  [A loss: 0.987151, acc: 0.257812]\n",
      "911: [D loss: 0.591941, acc: 0.681641]  [A loss: 1.097309, acc: 0.152344]\n",
      "912: [D loss: 0.611900, acc: 0.671875]  [A loss: 0.924782, acc: 0.300781]\n",
      "913: [D loss: 0.590942, acc: 0.669922]  [A loss: 1.114283, acc: 0.164062]\n",
      "914: [D loss: 0.602753, acc: 0.679688]  [A loss: 0.948285, acc: 0.277344]\n",
      "915: [D loss: 0.597230, acc: 0.695312]  [A loss: 1.118226, acc: 0.164062]\n",
      "916: [D loss: 0.649622, acc: 0.628906]  [A loss: 0.882420, acc: 0.328125]\n",
      "917: [D loss: 0.610001, acc: 0.666016]  [A loss: 1.301773, acc: 0.082031]\n",
      "918: [D loss: 0.634096, acc: 0.638672]  [A loss: 0.712142, acc: 0.515625]\n",
      "919: [D loss: 0.630623, acc: 0.652344]  [A loss: 1.330221, acc: 0.054688]\n",
      "920: [D loss: 0.642713, acc: 0.630859]  [A loss: 0.873639, acc: 0.359375]\n",
      "921: [D loss: 0.579124, acc: 0.718750]  [A loss: 1.063200, acc: 0.175781]\n",
      "922: [D loss: 0.611148, acc: 0.677734]  [A loss: 0.975903, acc: 0.210938]\n",
      "923: [D loss: 0.565216, acc: 0.724609]  [A loss: 0.977657, acc: 0.238281]\n",
      "924: [D loss: 0.569972, acc: 0.712891]  [A loss: 1.029954, acc: 0.207031]\n",
      "925: [D loss: 0.586709, acc: 0.701172]  [A loss: 1.058263, acc: 0.195312]\n",
      "926: [D loss: 0.609975, acc: 0.685547]  [A loss: 0.853162, acc: 0.402344]\n",
      "927: [D loss: 0.598726, acc: 0.679688]  [A loss: 1.347574, acc: 0.078125]\n",
      "928: [D loss: 0.637514, acc: 0.625000]  [A loss: 0.797229, acc: 0.433594]\n",
      "929: [D loss: 0.604352, acc: 0.697266]  [A loss: 1.150847, acc: 0.167969]\n",
      "930: [D loss: 0.611673, acc: 0.669922]  [A loss: 0.844954, acc: 0.375000]\n",
      "931: [D loss: 0.608497, acc: 0.667969]  [A loss: 1.186569, acc: 0.140625]\n",
      "932: [D loss: 0.619375, acc: 0.640625]  [A loss: 0.876300, acc: 0.339844]\n",
      "933: [D loss: 0.630408, acc: 0.638672]  [A loss: 1.045225, acc: 0.171875]\n",
      "934: [D loss: 0.621537, acc: 0.658203]  [A loss: 0.930480, acc: 0.312500]\n",
      "935: [D loss: 0.593524, acc: 0.689453]  [A loss: 1.009567, acc: 0.210938]\n",
      "936: [D loss: 0.606508, acc: 0.687500]  [A loss: 0.969141, acc: 0.238281]\n",
      "937: [D loss: 0.599359, acc: 0.677734]  [A loss: 1.015621, acc: 0.195312]\n",
      "938: [D loss: 0.606143, acc: 0.669922]  [A loss: 0.982808, acc: 0.250000]\n",
      "939: [D loss: 0.595698, acc: 0.681641]  [A loss: 1.134251, acc: 0.128906]\n",
      "940: [D loss: 0.626835, acc: 0.638672]  [A loss: 0.912832, acc: 0.300781]\n",
      "941: [D loss: 0.582806, acc: 0.697266]  [A loss: 1.180484, acc: 0.148438]\n",
      "942: [D loss: 0.611597, acc: 0.662109]  [A loss: 0.897159, acc: 0.312500]\n",
      "943: [D loss: 0.610011, acc: 0.693359]  [A loss: 1.139379, acc: 0.148438]\n",
      "944: [D loss: 0.606452, acc: 0.650391]  [A loss: 0.934874, acc: 0.289062]\n",
      "945: [D loss: 0.596071, acc: 0.666016]  [A loss: 1.110022, acc: 0.183594]\n",
      "946: [D loss: 0.613994, acc: 0.640625]  [A loss: 0.901963, acc: 0.324219]\n",
      "947: [D loss: 0.609492, acc: 0.658203]  [A loss: 1.199902, acc: 0.121094]\n",
      "948: [D loss: 0.626483, acc: 0.654297]  [A loss: 0.795363, acc: 0.414062]\n",
      "949: [D loss: 0.606343, acc: 0.658203]  [A loss: 1.278257, acc: 0.093750]\n",
      "950: [D loss: 0.634785, acc: 0.630859]  [A loss: 0.856904, acc: 0.355469]\n",
      "951: [D loss: 0.654136, acc: 0.646484]  [A loss: 1.240630, acc: 0.085938]\n",
      "952: [D loss: 0.642745, acc: 0.628906]  [A loss: 0.794042, acc: 0.414062]\n",
      "953: [D loss: 0.603832, acc: 0.658203]  [A loss: 1.057503, acc: 0.191406]\n",
      "954: [D loss: 0.583248, acc: 0.689453]  [A loss: 0.912123, acc: 0.292969]\n",
      "955: [D loss: 0.601092, acc: 0.666016]  [A loss: 1.114005, acc: 0.179688]\n",
      "956: [D loss: 0.632511, acc: 0.656250]  [A loss: 0.955480, acc: 0.230469]\n",
      "957: [D loss: 0.638208, acc: 0.652344]  [A loss: 1.094816, acc: 0.148438]\n",
      "958: [D loss: 0.591689, acc: 0.683594]  [A loss: 0.971586, acc: 0.257812]\n",
      "959: [D loss: 0.608595, acc: 0.656250]  [A loss: 0.995751, acc: 0.242188]\n",
      "960: [D loss: 0.640085, acc: 0.650391]  [A loss: 0.920380, acc: 0.285156]\n",
      "961: [D loss: 0.599713, acc: 0.683594]  [A loss: 0.944092, acc: 0.265625]\n",
      "962: [D loss: 0.594279, acc: 0.660156]  [A loss: 0.913108, acc: 0.292969]\n",
      "963: [D loss: 0.603380, acc: 0.685547]  [A loss: 1.104867, acc: 0.156250]\n",
      "964: [D loss: 0.587951, acc: 0.687500]  [A loss: 0.823119, acc: 0.378906]\n",
      "965: [D loss: 0.652509, acc: 0.642578]  [A loss: 1.186973, acc: 0.097656]\n",
      "966: [D loss: 0.604936, acc: 0.677734]  [A loss: 0.818077, acc: 0.343750]\n",
      "967: [D loss: 0.632328, acc: 0.630859]  [A loss: 1.182852, acc: 0.113281]\n",
      "968: [D loss: 0.628362, acc: 0.662109]  [A loss: 0.828815, acc: 0.375000]\n",
      "969: [D loss: 0.612084, acc: 0.691406]  [A loss: 1.230170, acc: 0.085938]\n",
      "970: [D loss: 0.630740, acc: 0.652344]  [A loss: 0.850633, acc: 0.339844]\n",
      "971: [D loss: 0.621728, acc: 0.658203]  [A loss: 1.107024, acc: 0.156250]\n",
      "972: [D loss: 0.605165, acc: 0.673828]  [A loss: 0.870665, acc: 0.300781]\n",
      "973: [D loss: 0.591772, acc: 0.703125]  [A loss: 1.030096, acc: 0.179688]\n",
      "974: [D loss: 0.589097, acc: 0.710938]  [A loss: 1.010860, acc: 0.218750]\n",
      "975: [D loss: 0.592313, acc: 0.693359]  [A loss: 0.981469, acc: 0.246094]\n",
      "976: [D loss: 0.599461, acc: 0.667969]  [A loss: 1.182989, acc: 0.113281]\n",
      "977: [D loss: 0.604720, acc: 0.685547]  [A loss: 0.890225, acc: 0.351562]\n",
      "978: [D loss: 0.625510, acc: 0.636719]  [A loss: 1.042406, acc: 0.160156]\n",
      "979: [D loss: 0.607935, acc: 0.687500]  [A loss: 0.936757, acc: 0.296875]\n",
      "980: [D loss: 0.641300, acc: 0.630859]  [A loss: 1.032860, acc: 0.179688]\n",
      "981: [D loss: 0.585775, acc: 0.687500]  [A loss: 0.927160, acc: 0.300781]\n",
      "982: [D loss: 0.601885, acc: 0.654297]  [A loss: 1.076163, acc: 0.148438]\n",
      "983: [D loss: 0.593079, acc: 0.697266]  [A loss: 0.930994, acc: 0.335938]\n",
      "984: [D loss: 0.614601, acc: 0.662109]  [A loss: 1.181931, acc: 0.125000]\n",
      "985: [D loss: 0.590084, acc: 0.695312]  [A loss: 0.914725, acc: 0.359375]\n",
      "986: [D loss: 0.670235, acc: 0.599609]  [A loss: 1.390844, acc: 0.054688]\n",
      "987: [D loss: 0.642558, acc: 0.640625]  [A loss: 0.822382, acc: 0.367188]\n",
      "988: [D loss: 0.646636, acc: 0.619141]  [A loss: 1.152187, acc: 0.148438]\n",
      "989: [D loss: 0.630168, acc: 0.630859]  [A loss: 0.874619, acc: 0.332031]\n",
      "990: [D loss: 0.600577, acc: 0.677734]  [A loss: 1.105180, acc: 0.140625]\n",
      "991: [D loss: 0.578027, acc: 0.714844]  [A loss: 1.008827, acc: 0.183594]\n",
      "992: [D loss: 0.601427, acc: 0.673828]  [A loss: 1.092167, acc: 0.121094]\n",
      "993: [D loss: 0.600471, acc: 0.685547]  [A loss: 0.965153, acc: 0.269531]\n",
      "994: [D loss: 0.578875, acc: 0.683594]  [A loss: 1.044663, acc: 0.199219]\n",
      "995: [D loss: 0.595396, acc: 0.705078]  [A loss: 1.002960, acc: 0.238281]\n",
      "996: [D loss: 0.574648, acc: 0.710938]  [A loss: 1.004971, acc: 0.207031]\n",
      "997: [D loss: 0.600141, acc: 0.687500]  [A loss: 0.946793, acc: 0.242188]\n",
      "998: [D loss: 0.588006, acc: 0.707031]  [A loss: 1.169440, acc: 0.125000]\n",
      "999: [D loss: 0.591333, acc: 0.697266]  [A loss: 0.848102, acc: 0.335938]\n",
      "Elapsed: 9.148986518383026 min \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timer = ElapsedTimer()\n",
    "train(train_epochs=1000, batch_size=256, save_interval=100) \n",
    "timer.elapsed_time()\n",
    "plot_images(fake=True)\n",
    "plot_images(fake=False, saveToFile=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
